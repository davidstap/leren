{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren: Programming assignment 3\n",
    "** This assignment can be done in teams of 2 **\n",
    "\n",
    "**Student 1:**  <span style=\"color:red\">Tycho Koster</span> (<span style=\"color:red\">10667687</span>)<br>\n",
    "**Student 2:** <span style=\"color:red\">David Stap</span> (<span style=\"color:red\">10608516</span>)<br>\n",
    "\n",
    "-----------------------------------\n",
    "This notebook provides a template for your programming assignment 3. You may want to use parts of your code from the previous assignment(s) as a starting point for this assignment. \n",
    "\n",
    "The code you hand-in should follow the structure from this document. Each part of the assignment has its own cell, you are free to add more cells. Note that the structure corresponds with the structure from the actual programming assignment. Make sure you read this for the full explanation of what is expected of you.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "* Make sure your code can be run from top to bottom without errors.\n",
    "* Include your data files in the zip file.\n",
    "* Comment your code\n",
    "\n",
    "One way be sure you code can be run without errors is by quiting iPython completely and then restart iPython and run all cells again (you can do this by going to the menu bar above: Cell > Run all). This way you make sure that no old definitions of functions or values of variables are left (that your program might still be using).\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "If you have any questions ask your teaching assistent. We are here for you.\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression\n",
    "a) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add data\n",
    "# X1  X2  X3  Y\n",
    "# 10  4   4   0\n",
    "# 7   3   3   0 \n",
    "# 5   4   2   1\n",
    "# 2   3   1   1\n",
    "def read_logistic_data():\n",
    "    x = []\n",
    "    y= []\n",
    "    y.append(0)\n",
    "    y.append(0)\n",
    "    y.append(1)\n",
    "    y.append(1)\n",
    "    x.append([10.0, 4.0, 4.0])\n",
    "    x.append([7.0, 3.0, 3.0])\n",
    "    x.append([5.0, 4.0, 2.0])\n",
    "    x.append([2.0, 3.0, 1.0])    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_theta_array(x):\n",
    "    theta = [0.5]\n",
    "    for i in range(len(x)):\n",
    "        theta.append(0.5)\n",
    "    return theta\n",
    "\n",
    "\n",
    "x, y = read_logistic_data()\n",
    "theta_array = create_theta_array(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "gradient_function_logistic\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values)\n",
    "Output: gradient for every theta in array\n",
    "'''\n",
    "\n",
    "def gradient_function_logistic(alpha, theta_array, theta, x, y):\n",
    "    m = len(x)\n",
    "    totalsum = 0\n",
    "    for i in range(0, len(x[0])):\n",
    "        temp_x = []\n",
    "        #temporary x for the values of all features at that specific index\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        if theta == 0:\n",
    "            totalsum += (Hx_logistic(theta_array, temp_x) - y[i])\n",
    "        else:\n",
    "            totalsum += (Hx_logistic(theta_array, temp_x) - y[i])*x[theta-1][i]\n",
    "    return (alpha*totalsum)/m\n",
    "    \n",
    "'''\n",
    "Hx_logistic\n",
    "Input: theta_array (list of theta values), x (list of values)\n",
    "Output: prediction of y with logistic hypotheses calculations\n",
    "'''\n",
    "def Hx_logistic(theta_array, x):\n",
    "    exponent = theta_array[0]\n",
    "    for i in range(1, len(theta_array)):\n",
    "        exponent += theta_array[i]*x[i-1]\n",
    "    e = -exponent\n",
    "    return 1.0/(1 + math.exp(e))\n",
    "\n",
    "'''\n",
    "update_theta_logistic\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values),\n",
    "iterations (number of iterations to be done)\n",
    "Output: All values of theta after the amount of iterations with the use of logistic regression.\n",
    "'''\n",
    "def update_theta_logistic(alpha, theta_array, x, y, iterations):\n",
    "    costs = []\n",
    "    costs.append(cost(theta_array, x, y))\n",
    "    for i in range(iterations):\n",
    "        for j in range(len(theta_array)):\n",
    "            theta_array[j] -= gradient_function_logistic(alpha, theta_array,j, x, y)\n",
    "        costs.append(cost(theta_array, x, y))\n",
    "    return theta_array, costs\n",
    "\n",
    "def hypothesis(theta, training_example):\n",
    "    totalsum = 0.0\n",
    "    for i in range(len(theta[1:])):\n",
    "        totalsum += theta[i]*training_example[i]\n",
    "    # + theta_0 * 1\n",
    "    totalsum+=theta[0]  \n",
    "    return 1.0/(1 + math.exp(-totalsum))\n",
    "    \n",
    "    \n",
    "def cost(theta, x, y):\n",
    "    cost = 0.0\n",
    "    for t in range(len(x)):\n",
    "        if y[t] == 1:\n",
    "            cost += math.log10(hypothesis(theta, x[t]))\n",
    "        if y[t] == 0:\n",
    "            cost += math.log10(1-hypothesis(theta, x[t]))\n",
    "        \n",
    "    return (-1.0/len(x)) * cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.49502293901017974,\n",
       "  0.46509236400983045,\n",
       "  0.4750796202086097,\n",
       "  0.4775725918897131],\n",
       " [1.7950707304149853, 1.7399201079007602])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_theta_logistic(0.01, theta_array, x, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_theta(alpha, theta, x, y, iterations):\n",
    "    \n",
    "    errors = []\n",
    "    for t in range(len(x)):\n",
    "        errors.append(hypothesis(theta, x[t]) - y[t])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return errors\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999251537724895,\n",
       " 0.9990889488055994,\n",
       " -0.002472623156634657,\n",
       " -0.02931223075135636]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_theta(0.01, theta_array, x, y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999925153772\n",
      "0.999088948806\n",
      "0.997527376843\n",
      "0.970687769249\n"
     ]
    }
   ],
   "source": [
    "print hypothesis(theta_array, x[0])\n",
    "print hypothesis(theta_array, x[1])\n",
    "print hypothesis(theta_array, x[2])\n",
    "print hypothesis(theta_array, x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.495081926878\n"
     ]
    }
   ],
   "source": [
    "x1 = hypothesis(theta_array, x[0])\n",
    "x2 = hypothesis(theta_array, x[1])\n",
    "x3 = hypothesis(theta_array, x[2])-1\n",
    "x4 = hypothesis(theta_array, x[3])-1\n",
    "\n",
    "print 0.5 - 0.01 * (1/4.0)*(x1+x2+x3+x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Two small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "[You discussion comes here]\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate steps:  [array([[-5]]), array([ 0.26894142]), array([ 0.50672313])]\n",
      "Y activation:  [ 0.50672313]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Input: \n",
    "- Input value(s) X\n",
    "- List with weights for hidden layer(s) W\n",
    "- Weights for final layer W_output (can be of a different dimension than\n",
    "  W, because n of output values can be different than n of nodes in \n",
    "  hidden layers)\n",
    "Output: \n",
    "- activation value(s) of y \n",
    "'''\n",
    "def forward(X, W, W_output):    \n",
    "    steps = list()\n",
    "    steps.append(X)    \n",
    "    # loop over weights for hidden layers\n",
    "    for i in range(len(W)):\n",
    "        steps.append(sigmoid(np.dot(steps[i], W[i])))    \n",
    "    # for final value, use W_output\n",
    "    steps.append(sigmoid(np.dot(steps[-1], W_output)))\n",
    "    \n",
    "    return steps, steps[-1]\n",
    "  \n",
    "    \n",
    "#example run: calculate activation of Y for neural network in written3\n",
    "X = np.array([[-5]])\n",
    "W = np.array([[0.2]])\n",
    "W_output = np.array([[0.1]])\n",
    "\n",
    "steps, result = forward(X, W, W_output)\n",
    "print 'intermediate steps: ', steps\n",
    "print 'Y activation: ', result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y activation:  [ 0.00074474  0.45844378  0.62710019]\n"
     ]
    }
   ],
   "source": [
    "# more fancy example run: calculate activation of y where:\n",
    "\n",
    "inputLayerSize = 10\n",
    "outputLayerSize = 3\n",
    "hiddenLayerSize = 35\n",
    "hiddenLayerDepth = 1\n",
    "\n",
    "def random_weights(inputLayerSize, outputLayerSize, hiddenLayerSize, hiddenLayerDepth):    \n",
    "    # create random weight values for all hidden layers\n",
    "    W = list()\n",
    "    W.append(np.random.randn(inputLayerSize, hiddenLayerSize))\n",
    "    if hiddenLayerSize > 1:\n",
    "        for i in range(hiddenLayerDepth):\n",
    "            W.append(np.random.randn(hiddenLayerSize, hiddenLayerSize))\n",
    "\n",
    "    # W_output might have different dimensions \n",
    "    # (if outputLayerSize != hiddenLayerSize)\n",
    "    W_output = np.random.randn(hiddenLayerSize, outputLayerSize)\n",
    "    \n",
    "    return W, W_output\n",
    "\n",
    "X = np.array([0,0,0,12,13,5,0,0,0,0])\n",
    "\n",
    "W, W_output = random_weights(len(X), outputLayerSize, hiddenLayerSize, hiddenLayerDepth)\n",
    "\n",
    "steps, result = forward(X, W, W_output)\n",
    "print 'Y activation: ', result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Backpropagation on two logistic units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhwAAAF5CAYAAADUL/MIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb44d2eaa10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  [ 14.69321372  -1.65275945]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "backpropagation for data from written assignment 1:\n",
    "X  |  6  5  3\n",
    "Y  |  5  6  10\n",
    "\n",
    "Input:\n",
    "- X (training examples + added row of ones for bias unit)\n",
    "- y (y values)\n",
    "- w (list with weights)\n",
    "- alpha (learning rate)\n",
    "- number of iterations\n",
    "Output:\n",
    "- weights after iterations\n",
    "'''\n",
    "def error_backpropagation(X, y, w, alpha, iterations):\n",
    "    for t in range(iterations):\n",
    "        grad = np.array([0., 0.])\n",
    "        # loop through training examples\n",
    "        for i in range(len(X)):\n",
    "            x_i = X[i, :]\n",
    "            y_i = y[i]\n",
    "            h = np.dot(w, x_i)-y_i\n",
    "            grad += 2*x_i*h\n",
    "\n",
    "        # update weights\n",
    "        w = w - alpha * grad\n",
    "\n",
    "    plot(X, y, w) \n",
    "    return w\n",
    "\n",
    "def plot(X, y, w):   \n",
    "    tt = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 10)\n",
    "    bf_line = w[0]+w[1]*tt\n",
    "    plt.plot(X[:, 1], y, 'kx', tt, bf_line, 'r-')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "X = np.array([[1,6], [1,5], [1,3]])\n",
    "y = np.array([5,6,10])\n",
    "w = np.array([0.5, 0.5])\n",
    "\n",
    "weights = error_backpropagation(X, y, w, 0.001, 10000)\n",
    "print \"weights: \", weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current output:  [ 0.50672313]\n",
      "new output:  [ 0.51463352]\n",
      "---------------\n",
      "current output:  [ 0.51463352]\n",
      "new output:  [ 0.52818891]\n",
      "---------------\n",
      "current output:  [ 0.52818891]\n",
      "new output:  [ 0.55496235]\n",
      "---------------\n",
      "current output:  [ 0.55496235]\n",
      "new output:  [ 0.60284722]\n",
      "---------------\n",
      "current output:  [ 0.60284722]\n",
      "new output:  [ 0.66075745]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "backpropagation for written3 assignment 3.3\n",
    "\n",
    "Input:\n",
    "- X (training examples)\n",
    "- y (y values)\n",
    "- W (list with weights for hidden layers)\n",
    "- W_output (weights for final hidden layer)\n",
    "- alpha (learning rate)\n",
    "- number of iterations\n",
    "'''\n",
    "def backpropagation_written3_3_3(X, y, W, W_output, alpha, iterations):\n",
    "    steps, result = forward(X, W, W_output)\n",
    "    if iterations > 0:\n",
    "\n",
    "        print 'current output: ', result\n",
    "        ### LAST WEIGHTS\n",
    "        d_2 = result - y\n",
    "\n",
    "        ### MIDDLE WEIGHTS\n",
    "        d_1 = W_output * d_2 * sigmoid_derivative(steps[-2])    \n",
    "        dy_dtheta2 = steps[-2] * d_2\n",
    "\n",
    "        # FIRST WEIGHTS\n",
    "        dy_dtheta1 = X[0] * d_1\n",
    "\n",
    "        ### UPDATE WEIGHTS\n",
    "        W_updated = W - alpha * dy_dtheta1\n",
    "        W_output_updated = W_output - alpha * dy_dtheta2\n",
    "\n",
    "        steps, result = forward(X, W_updated, W_output_updated)\n",
    "        print 'new output: ', result\n",
    "        print '---------------'\n",
    "        \n",
    "        backpropagation_written3_3_3(X, y, W_updated, W_output_updated, alpha, iterations-1)\n",
    "\n",
    "\n",
    "X = np.array([[-5]])\n",
    "y = np.array([[1]])\n",
    "W = np.array([[0.2]])\n",
    "W_output = np.array([[0.1]])\n",
    "\n",
    "backpropagation_written3_3_3(X, y, W, W_output, 0.7, 5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Complete backpropagation on handwritten digit recognition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "### QUESTION 2B ###\n",
    "\n",
    "def backpropagation performs gradient descent using a neural network to find the best weights for theta_0 and theta_1. One input value is chosen, namely X. Also, a bias unit is added; this way we can make sure that the resulting formula for a line does not have to go through the origin. \n",
    "\n",
    "def backpropagation_written_3_3_3 is an additional function that calculates the values from written3 assignment 3.3. From the results it can be seen that the output correctly increases (because Y = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "# import X and y from csv file\n",
    "def read_logistic_data():\n",
    "    my_data = genfromtxt('digits123.csv', delimiter=',')\n",
    "    X = []\n",
    "    y= []\n",
    "    for i in range(len(my_data)):\n",
    "        new_data = my_data[i]\n",
    "        y.append([new_data[-1]])\n",
    "        X.append(np.delete(new_data, -1))\n",
    "    return X, y\n",
    "\n",
    "X, temp_y = read_logistic_data()\n",
    "\n",
    "'''\n",
    "for this specific handwritten digit classification problem, we need to have \n",
    "three classes: 1, 2, 3. Representation of the classes will be as follows:\n",
    "\n",
    "class '1' =   class '2' =   class '3' =\n",
    "[1,           [0,           [0,\n",
    " 0,            1,            0,\n",
    " 0]            0]            1]\n",
    "'''\n",
    "y = []\n",
    "\n",
    "for i in range(len(temp_y)):\n",
    "    if temp_y[i][0] == 1.0:\n",
    "        y.append(np.array([[1, 0, 0]]))        \n",
    "    if temp_y[i][0] == 2.0:\n",
    "        y.append(np.array([[0, 1, 0]]))      \n",
    "    if temp_y[i][0] == 3.0:\n",
    "        y.append(np.array([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37136759  0.97767139  0.85072009]\n"
     ]
    }
   ],
   "source": [
    "# chosen parameters for neural network:\n",
    "inputLayerSize = 64   # 64 features\n",
    "outputLayerSize = 3   # three classes: 1, 2, 3\n",
    "hiddenLayerSize = 35  # simple rule of thumb: n of neurons in hidden layer\n",
    "                      # is mean of inputLayerSize and outPutlayerSize.\n",
    "hiddenLayerDepth = 1\n",
    "\n",
    "# create random weights for NN using above parameters\n",
    "W, W_output = random_weights(inputLayerSize, outputLayerSize, hiddenLayerSize, hiddenLayerDepth)\n",
    "\n",
    "\n",
    "\n",
    "steps, result = forward(X[0], W, W_output)\n",
    "\n",
    "print result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62863241,  0.97767139,  0.85072009]])"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result-y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current output:  [ 0.37136759  0.97767139  0.85072009]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (35,3) (35,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-688-9d3f662ee9c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#backpropagation_written3_3_3(X, y, W, W_output, alpha, iterations)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbackpropagation_written3_3_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-501-fa90b430be28>\u001b[0m in \u001b[0;36mbackpropagation_written3_3_3\u001b[1;34m(X, y, W, W_output, alpha, iterations)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m### MIDDLE WEIGHTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0md_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW_output\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msigmoid_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mdy_dtheta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (35,3) (35,) "
     ]
    }
   ],
   "source": [
    "#backpropagation_written3_3_3(X, y, W, W_output, alpha, iterations)\n",
    "\n",
    "backpropagation_written3_3_3(X[0], y[0], W, W_output, 0.01, 1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
