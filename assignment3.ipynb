{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren: Programming assignment 3\n",
    "** This assignment can be done in teams of 2 **\n",
    "\n",
    "**Student 1:**  <span style=\"color:red\">Tycho Koster</span> (<span style=\"color:red\">10667687</span>)<br>\n",
    "**Student 2:** <span style=\"color:red\">David Stap</span> (<span style=\"color:red\">10608516</span>)<br>\n",
    "\n",
    "-----------------------------------\n",
    "This notebook provides a template for your programming assignment 3. You may want to use parts of your code from the previous assignment(s) as a starting point for this assignment. \n",
    "\n",
    "The code you hand-in should follow the structure from this document. Each part of the assignment has its own cell, you are free to add more cells. Note that the structure corresponds with the structure from the actual programming assignment. Make sure you read this for the full explanation of what is expected of you.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "* Make sure your code can be run from top to bottom without errors.\n",
    "* Include your data files in the zip file.\n",
    "* Comment your code\n",
    "\n",
    "One way be sure you code can be run without errors is by quiting iPython completely and then restart iPython and run all cells again (you can do this by going to the menu bar above: Cell > Run all). This way you make sure that no old definitions of functions or values of variables are left (that your program might still be using).\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "If you have any questions ask your teaching assistent. We are here for you.\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression\n",
    "a) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add data\n",
    "# X1  X2  X3  Y\n",
    "# 10  4   4   0\n",
    "# 7   3   3   0 \n",
    "# 5   4   2   1\n",
    "# 2   3   1   1\n",
    "def read_logistic_data():\n",
    "    x = []\n",
    "    y= []\n",
    "    y.append(0)\n",
    "    y.append(0)\n",
    "    y.append(1)\n",
    "    y.append(1)\n",
    "    x.append([10.0, 7.0,5.0,2.0])\n",
    "    x.append([4.0, 3.0, 4.0, 3.0])\n",
    "    x.append([4.0, 3.0, 2.0, 1.0])  \n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_theta_array(x):\n",
    "    theta = [0.5]\n",
    "    for i in range(len(x)):\n",
    "        theta.append(0.5)\n",
    "    return theta\n",
    "\n",
    "\n",
    "x, y = read_logistic_data()\n",
    "theta_array = create_theta_array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "gradient_function_logistic\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values), \n",
    "lam (regularization variable called lambda)\n",
    "Output: gradient for every theta in array\n",
    "'''\n",
    "\n",
    "def gradient_function_logistic(alpha, theta_array, theta, x, y, lam):\n",
    "    m = len(x)\n",
    "    totalsum = 0\n",
    "    for i in range(0, len(x[0])):\n",
    "        temp_x = []\n",
    "        #temporary x for the values of all features at that specific index\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        if theta == 0:\n",
    "            totalsum += (Hx_logistic(theta_array, temp_x) - y[i])\n",
    "        else:\n",
    "            totalsum += (Hx_logistic(theta_array, temp_x) - y[i])*x[theta-1][i] + (lam/m)*theta_array[theta]\n",
    "    return (alpha*totalsum)/m\n",
    "    \n",
    "'''\n",
    "Hx_logistic\n",
    "Input: theta_array (list of theta values), x (list of values)\n",
    "Output: prediction of y with logistic hypotheses calculations\n",
    "'''\n",
    "def Hx_logistic(theta_array, x):\n",
    "    exponent = theta_array[0]\n",
    "    for i in range(1, len(theta_array)):\n",
    "        exponent += theta_array[i]*x[i-1]\n",
    "    e = -exponent\n",
    "    return 1.0/(1 + math.exp(e))\n",
    "\n",
    "'''\n",
    "update_theta_logistic\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values),\n",
    "iterations (number of iterations to be done), lam (regularization variable called lambda)\n",
    "Output: All values of theta after the amount of iterations with the use of logistic regression.\n",
    "'''\n",
    "def update_theta_logistic(alpha, theta_array, x, y, iterations, lam):\n",
    "    costs = []\n",
    "    costs.append(cost_function_logistic(theta_array, x, y, lam))\n",
    "    for i in range(iterations):\n",
    "        for j in range(len(theta_array)):\n",
    "            theta_array[j] -= gradient_function_logistic(alpha, theta_array,j, x, y,lam)\n",
    "        costs.append(cost_function_logistic(theta_array, x, y, lam))\n",
    "    return theta_array, costs\n",
    "\n",
    "'''\n",
    "cost_function_logistic\n",
    "Input: theta_array (list of theta values), x (list of multiple features with values), y (list of values)\n",
    "Output: The cost of the theta values with logistic regression.\n",
    "'''\n",
    "def cost_function_logistic(theta_array, x, y, lam):\n",
    "    m = len(x[0])\n",
    "    totalsum = 0\n",
    "    regsum = 0\n",
    "    for i in range(len(x[0])):\n",
    "        temp_x = []\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        if(y[i] == 1):\n",
    "            totalsum += math.log10(Hx_logistic(theta_array, temp_x))\n",
    "        else:\n",
    "            totalsum += math.log10(1 - Hx_logistic(theta_array, temp_x))\n",
    "        if(i != 0):\n",
    "            regsum += theta_array[i]**2\n",
    "    cost = -(1.0/m)*(totalsum)\n",
    "    regcost = (lam/(2*m))*regsum\n",
    "    return cost + regcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta array:\n",
      "[0.4676100856139274, 0.21927812358551907, 0.3868218062359511, 0.384922572224571]\n",
      "Cost array:\n",
      "[1.7950707304149853, 1.6554548811715786, 1.5164253042078095, 1.3782109451928626, 1.2411554407953922, 1.105784538018239]\n",
      "Theta array:\n",
      "[0.4677171016793583, 0.1946150220736454, 0.3578447355339643, 0.3558086844584255]\n",
      "Cost array:\n",
      "[1.7950707304149853, 1.6332341486262159, 1.4743218667953464, 1.3186402360485057, 1.1666805280668868, 1.0192321839390655]\n",
      "Theta array:\n",
      "[0.46797337298990355, 0.14995351216740532, 0.3048955142645279, 0.3025627761979979]\n",
      "Cost array:\n",
      "[2.5450707304149853, 2.185883025311374, 1.860593758441854, 1.567172085360498, 1.3042241593658381, 1.071210299903874]\n",
      "Theta array:\n",
      "[0.46848526296379805, 0.09383877746177971, 0.2371061141435325, 0.2342897647243424]\n",
      "Cost array:\n",
      "[3.2950707304149853, 2.611894814674272, 2.04748834713378, 1.5847217523947499, 1.2103950945362048, 0.9150225668570028]\n"
     ]
    }
   ],
   "source": [
    "thetas, costs = update_theta_logistic(0.01, [0.5,0.5,0.5,0.5], x, y, 5, 1)\n",
    "print(\"Theta array:\")\n",
    "print(thetas)\n",
    "print(\"Cost array:\")\n",
    "print(costs)\n",
    "thetas, costs = update_theta_logistic(0.01, [0.5,0.5,0.5,0.5], x, y, 5, 5)\n",
    "print(\"Theta array:\")\n",
    "print(thetas)\n",
    "print(\"Cost array:\")\n",
    "print(costs)\n",
    "thetas, costs = update_theta_logistic(0.01, [0.5,0.5,0.5,0.5], x, y, 5, 10)\n",
    "print(\"Theta array:\")\n",
    "print(thetas)\n",
    "print(\"Cost array:\")\n",
    "print(costs)\n",
    "thetas, costs = update_theta_logistic(0.01, [0.5,0.5,0.5,0.5], x, y, 5, 20)\n",
    "print(\"Theta array:\")\n",
    "print(thetas)\n",
    "print(\"Cost array:\")\n",
    "print(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Two small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1, lambda 10:\n",
      "[0.48333393397321095, 0.3769174072044312, 0.37691759822534837, 0.3153752546952773]\n",
      "[1.3483623209398385, 2.2405165372950107, 1.9702086692356358, 1.7278580873418512, 1.5110068915446093, 1.3174096394766437]\n",
      "Dataset 1, lambda 20:\n",
      "[0.4833387327556216, 0.3011501785353169, 0.3011547444273259, 0.24433914106292123]\n",
      "[2.0983623209398385, 2.681904552765869, 2.1773880982985094, 1.760886290542037, 1.418117277382204, 1.1372060784827966]\n",
      "Dataset 1, lambda 50:\n",
      "[0.4833836905807097, 0.2190917221663012, 0.21913484459417723, 0.16809571416161523]\n",
      "[2.8483623209398385, 2.939183191907527, 2.127570609398261, 1.5323323549478323, 1.0985897997569969, 0.7865126410682932]\n",
      "Dataset 2, lambda 10:\n",
      "[0.46666738304161043, 0.3769158516226776, 0.23844704907640216, 0.19229748235574637]\n",
      "[2.3246760194216627, 5.651641672486517, 4.440440696847019, 3.4202193064938955, 2.5721822245081167, 1.879440548326369]\n",
      "Dataset 2, lambda 20:\n",
      "[0.4666779644412624, 0.30114222457405077, 0.1733290837578151, 0.1308425154884171]\n",
      "[3.0746760194216627, 5.770521133108522, 4.162995161535264, 2.922800713738882, 1.9842331754877311, 1.296145731169209]\n",
      "Dataset 2, lambda 50:\n",
      "[0.46833940601439006, 0.13044164991794882, 0.039535770116008125, 0.015363198323726047]\n",
      "[6.074676019421663, 5.870943016219113, 3.045413603539859, 1.4915027989318899, 0.7337058431883088, 0.46475859667091507]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "gradient_function_logistic\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values), \n",
    "lam (regularization variable called lambda)\n",
    "Output: gradient for every theta in array\n",
    "'''\n",
    "\n",
    "def gradient_function_quadratic(alpha, theta_array, theta, x, y, lam):\n",
    "    m = len(x)\n",
    "    totalsum = 0\n",
    "    for i in range(0, len(x[0])):\n",
    "        temp_x = []\n",
    "        #temporary x for the values of all features at that specific index\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        if theta == 0:\n",
    "            totalsum += (Hx_quadratic(theta_array, temp_x) - y[i])\n",
    "        else:\n",
    "            totalsum += (Hx_quadratic(theta_array, temp_x) - y[i])*x[theta-1][i] + (lam/m)*theta_array[theta]\n",
    "    return (alpha*totalsum)/m\n",
    "    \n",
    "'''\n",
    "Hx_logistic\n",
    "Input: theta_array (list of theta values), x (list of values)\n",
    "Output: prediction of y with logistic quadratic hypotheses calculations\n",
    "'''\n",
    "def Hx_quadratic(theta_array, x):\n",
    "    exponent = theta_array[0]\n",
    "    for i in range(1, len(theta_array)):\n",
    "        exponent += (theta_array[i]*x[i-1])**2 + theta_array[i]*x[i-1]\n",
    "    e = -exponent\n",
    "    return 1.0/(1 + math.exp(e))\n",
    "'''\n",
    "update_theta_quadratic\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values),\n",
    "iterations (number of iterations to be done), lam (regularization variable called lambda)\n",
    "Output: All values of theta after the amount of iterations with the use of logistic regression with quadratic terms.\n",
    "'''\n",
    "def update_theta_quadratic(alpha, theta_array, x, y, iterations, lam):\n",
    "    costs = []\n",
    "    costs.append(cost_function_logistic(theta_array, x, y, lam))\n",
    "    for i in range(iterations):\n",
    "        for j in range(len(theta_array)):\n",
    "            theta_array[j] -= gradient_function_quadratic(alpha, theta_array,j, x, y,lam)\n",
    "        costs.append(cost_function_quadratic(theta_array, x, y, lam))\n",
    "    return theta_array, costs\n",
    "\n",
    "'''\n",
    "cost_function_quadratic\n",
    "Input: theta_array (list of theta values), x (list of multiple features with values), y (list of values), \n",
    "lam (regularization variable called lambda)\n",
    "Output: The cost of the theta values with logistic regression and the use of quadratic terms.\n",
    "'''\n",
    "def cost_function_quadratic(theta_array, x, y, lam):\n",
    "    m = len(x[0])\n",
    "    totalsum = 0\n",
    "    regsum = 0\n",
    "    for i in range(len(x[0])):\n",
    "        temp_x = []\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        if(y[i] == 1):\n",
    "            totalsum += math.log10(Hx_quadratic(theta_array, temp_x))\n",
    "        else:\n",
    "            totalsum += math.log10(1 - Hx_quadratic(theta_array, temp_x))\n",
    "        if(i != 0):\n",
    "            regsum += theta_array[i]**2\n",
    "    cost = -(1.0/m)*(totalsum)\n",
    "    regcost = (lam/(2*m))*regsum\n",
    "    return cost + regcost\n",
    "\n",
    "#dataset1\n",
    "# X1  X2  X3  Y\n",
    "# 2   2   6   0\n",
    "# 3   10  3   1 \n",
    "# 5   4   8   1\n",
    "# 5   3   1   1\n",
    "#dataset2\n",
    "# X1  X2  X3  Y\n",
    "# 1   8   6   0\n",
    "# 1   10  2   1 \n",
    "# 1   3   8   0\n",
    "# 1   3   10  1\n",
    "\n",
    "data1 = [[2,3,5,5], [2,10,4,3], [6,3,8,1], [0,1,1,1]]\n",
    "data2 = [[1,1,1,1], [8,10,3,3], [6,2,8,10], [0,1,0,1]]\n",
    "\n",
    "theta_array, costs = update_theta_quadratic(0.01, [0.5,0.5,0.5,0.5], data1[:-1], data1[-1], 5, 10)\n",
    "print(\"Dataset 1, lambda 10:\")\n",
    "print(theta_array)\n",
    "print(costs)\n",
    "theta_array, costs = update_theta_quadratic(0.01, [0.5,0.5,0.5,0.5], data1[:-1], data1[-1], 5, 20)\n",
    "print(\"Dataset 1, lambda 20:\")\n",
    "print(theta_array)\n",
    "print(costs)\n",
    "theta_array, costs = update_theta_quadratic(0.01, [0.5,0.5,0.5,0.5], data1[:-1], data1[-1], 5, 30)\n",
    "print(\"Dataset 1, lambda 50:\")\n",
    "print(theta_array)\n",
    "print(costs)\n",
    "theta_array, costs = update_theta_quadratic(0.01, [0.5,0.5,0.5,0.5], data2[:-1], data2[-1], 5, 10)\n",
    "print(\"Dataset 2, lambda 10:\")\n",
    "print(theta_array)\n",
    "print(costs)\n",
    "theta_array, costs = update_theta_quadratic(0.01, [0.5,0.5,0.5,0.5], data2[:-1], data2[-1], 5, 20)\n",
    "print(\"Dataset 2, lambda 20:\")\n",
    "print(theta_array)\n",
    "print(costs)\n",
    "theta_array, costs = update_theta_quadratic(0.01, [0.5,0.5,0.5,0.5], data2[:-1], data2[-1], 5, 50)\n",
    "print(\"Dataset 2, lambda 50:\")\n",
    "print(theta_array)\n",
    "print(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "***Question 1a***\n",
    "\n",
    "We used different values for the regularization variable lambda, this way we could see what the costs were with different values. It is noticable that for a higher lambda value the cost at the start is higher, but the cost after the same amount of iterations is less. How higher the lambda how faster the cost becomes less. The down side is that it starts with a higher cost.\n",
    "\n",
    "***Question 1b***\n",
    "\n",
    "We used different values for the regularization variable lambda, this way we could see what the costs were with different values. What we noticed with the use of quadratic terms is that with the first iteration the cost goes up. But with the use of a high lamba value this changes and the cost only goes down from the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate steps:  [array([[-5]]), array([ 0.26894142]), array([ 0.50672313])]\n",
      "Y activation:  [ 0.50672313]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Input: \n",
    "- Input value(s) X\n",
    "- List with weights for hidden layer(s) W\n",
    "- Weights for final layer W_output (can be of a different dimension than\n",
    "  W, because n of output values can be different than n of nodes in \n",
    "  hidden layers)\n",
    "Output: \n",
    "- activation value(s) of y \n",
    "'''\n",
    "def forward(X, W, W_output):    \n",
    "    steps = list()\n",
    "    steps.append(X)    \n",
    "    # loop over weights for hidden layers\n",
    "    for i in range(len(W)):\n",
    "        steps.append(sigmoid(np.dot(steps[i], W[i])))    \n",
    "    # for final value, use W_output\n",
    "    steps.append(sigmoid(np.dot(steps[-1], W_output)))\n",
    "    \n",
    "    return steps, steps[-1]\n",
    "  \n",
    "    \n",
    "#example run: calculate activation of Y for neural network in written3\n",
    "X = np.array([[-5]])\n",
    "W = np.array([[0.2]])\n",
    "W_output = np.array([[0.1]])\n",
    "\n",
    "steps, result = forward(X, W, W_output)\n",
    "print 'intermediate steps: ', steps\n",
    "print 'Y activation: ', result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y activation:  [ 0.3961714   0.99322813  0.59852368]\n",
      "Y activation:  [ 0.99999997  0.99999997  0.99999997]\n"
     ]
    }
   ],
   "source": [
    "# more fancy example run: calculate activation of y where:\n",
    "\n",
    "inputLayerSize = 10\n",
    "outputLayerSize = 3\n",
    "hiddenLayerSize = 35\n",
    "hiddenLayerDepth = 1\n",
    "\n",
    "\n",
    "'''\n",
    "Create weights for given dimensions.\n",
    "If value == -1, random weights will be created.\n",
    "If value == x, weights with value x will be created.\n",
    "'''\n",
    "\n",
    "def weights(inputLayerSize, outputLayerSize, hiddenLayerSize, hiddenLayerDepth, value):    \n",
    "    W = list()\n",
    "    W_output = 0\n",
    "    \n",
    "    if value == -1:\n",
    "        # create random weight values for all hidden layers\n",
    "        W = list()\n",
    "        W.append(np.random.randn(inputLayerSize, hiddenLayerSize))\n",
    "        if hiddenLayerDepth > 1:\n",
    "            for i in range(hiddenLayerDepth):\n",
    "                W.append(np.random.randn(hiddenLayerSize, hiddenLayerSize))\n",
    "\n",
    "        # W_output might have different dimensions \n",
    "        # (if outputLayerSize != hiddenLayerSize)\n",
    "        W_output = np.random.randn(hiddenLayerSize, outputLayerSize)\n",
    "    \n",
    "    if value != -1:\n",
    "        W = list()\n",
    "        W.append(np.full((inputLayerSize, hiddenLayerSize), value))\n",
    "        if hiddenLayerDepth > 1:\n",
    "            for i in range(hiddenLayerDepth):\n",
    "                W.append(np.full((hiddenLayerSize, hiddenLayerSize), value))\n",
    "            \n",
    "        # W_output might have different dimensions \n",
    "        # (if outputLayerSize != hiddenLayerSize)\n",
    "        W_output = np.full((hiddenLayerSize, outputLayerSize), value)\n",
    "        \n",
    "    return W, W_output\n",
    "\n",
    "X = np.array([0,0,0,12,13,5,0,0,0,0])\n",
    "\n",
    "W, W_output = weights(len(X), outputLayerSize, hiddenLayerSize, hiddenLayerDepth, -1)\n",
    "steps, result = forward(X, W, W_output)\n",
    "print 'Y activation: ', result\n",
    "\n",
    "W, W_output = weights(len(X), outputLayerSize, hiddenLayerSize, hiddenLayerDepth, 0.5)\n",
    "steps, result = forward(X, W, W_output)\n",
    "print 'Y activation: ', result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Backpropagation on two logistic units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhwAAAF5CAYAAADUL/MIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faffb672710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  [ 14.69321372  -1.65275945]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "backpropagation for data from written assignment 1:\n",
    "X  |  6  5  3\n",
    "Y  |  5  6  10\n",
    "\n",
    "Input:\n",
    "- X (training examples + added row of ones for bias unit)\n",
    "- y (y values)\n",
    "- w (list with weights)\n",
    "- alpha (learning rate)\n",
    "- number of iterations\n",
    "Output:\n",
    "- weights after iterations\n",
    "'''\n",
    "def error_backpropagation(X, y, w, alpha, iterations):\n",
    "    for t in range(iterations):\n",
    "        grad = np.array([0., 0.])\n",
    "        # loop through training examples\n",
    "        for i in range(len(X)):\n",
    "            x_i = X[i, :]\n",
    "            y_i = y[i]\n",
    "            h = np.dot(w, x_i)-y_i\n",
    "            grad += 2*x_i*h\n",
    "\n",
    "        # update weights\n",
    "        w = w - alpha * grad\n",
    "\n",
    "    plot(X, y, w) \n",
    "    return w\n",
    "\n",
    "def plot(X, y, w):   \n",
    "    tt = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 10)\n",
    "    bf_line = w[0]+w[1]*tt\n",
    "    plt.plot(X[:, 1], y, 'kx', tt, bf_line, 'r-')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "X = np.array([[1,6], [1,5], [1,3]])\n",
    "y = np.array([5,6,10])\n",
    "w = np.array([0.5, 0.5])\n",
    "\n",
    "weights = error_backpropagation(X, y, w, 0.001, 10000)\n",
    "print \"weights: \", weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2]] [[ 0.1]]\n",
      "current output:  [ 0.50672313]\n",
      "new output:  [ 0.50775062]\n",
      "---------------\n",
      "[[ 0.19515079]] [[ 0.11326626]]\n",
      "current output:  [ 0.50775062]\n",
      "new output:  [ 0.50884808]\n",
      "---------------\n",
      "[[ 0.18960861]] [[ 0.12674085]]\n",
      "current output:  [ 0.50884808]\n",
      "new output:  [ 0.51002828]\n",
      "---------------\n",
      "[[ 0.1833438]] [[ 0.14045767]]\n",
      "current output:  [ 0.51002828]\n",
      "new output:  [ 0.51130569]\n",
      "---------------\n",
      "[[ 0.17632261]] [[ 0.15445258]]\n",
      "current output:  [ 0.51130569]\n",
      "new output:  [ 0.51269678]\n",
      "---------------\n",
      "[[ 0.16850717]] [[ 0.16876368]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "backpropagation for written3 assignment 3.3\n",
    "\n",
    "Input:\n",
    "- X (training examples)\n",
    "- y (y values)\n",
    "- W (list with weights for hidden layers)\n",
    "- W_output (weights for final hidden layer)\n",
    "- alpha (learning rate)\n",
    "- number of iterations\n",
    "'''\n",
    "def backpropagation_written3_3_3(X, y, W, W_output, alpha, iterations):\n",
    "    steps, result = forward(X, W, W_output)\n",
    "    \n",
    "    print W, W_output\n",
    "    \n",
    "    if iterations > 0:\n",
    "\n",
    "        print 'current output: ', result\n",
    "        ### LAST WEIGHTS\n",
    "        d_2 = result - y\n",
    "        \n",
    "        ### MIDDLE WEIGHTS\n",
    "        d_1 = W_output * d_2 * sigmoid_derivative(steps[-2]) \n",
    "        \n",
    "        dy_dtheta2 = steps[-2] * d_2\n",
    "        \n",
    "        # FIRST WEIGHTS\n",
    "        dy_dtheta1 = X[0] * d_1\n",
    "\n",
    "        ### UPDATE WEIGHTS\n",
    "        W_updated = W - alpha * dy_dtheta1\n",
    "        W_output_updated = W_output - alpha * dy_dtheta2\n",
    "\n",
    "        steps, result = forward(X, W_updated, W_output_updated)\n",
    "        print 'new output: ', result\n",
    "        print '---------------'\n",
    "        \n",
    "        backpropagation_written3_3_3(X, y, W_updated, W_output_updated, alpha, iterations-1)\n",
    "\n",
    "\n",
    "X = np.array([[-5]])\n",
    "y = np.array([[1]])\n",
    "W = np.array([[0.2]])\n",
    "W_output = np.array([[0.1]])\n",
    "\n",
    "backpropagation_written3_3_3(X, y, W, W_output, 0.1, 5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "*** Question 2b ***\n",
    "\n",
    "def backpropagation performs gradient descent using a neural network to find the best weights for theta_0 and theta_1. One input value is chosen, namely X. Also, a bias unit is added; this way we can make sure that the resulting formula for a line does not have to go through the origin. \n",
    "\n",
    "def backpropagation_written_3_3_3 is an additional function that calculates the values from written3 assignment 3.3. From the results it can be seen that the output correctly increases (because Y = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Complete backpropagation on handwritten digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We removed the folowing lines from 'digits123.csv':\n",
    "\n",
    "- line 1-5 (class 1)\n",
    "0,0,0,12,13,5,0,0,0,0,0,11,16,9,0,0,0,0,3,15,16,6,0,0,0,7,15,16,16,2,0,0,0,0,1,16,16,3,0,0,0,0,1,16,16,6,0,0,0,0,1,16,16,6,0,0,0,0,0,11,16,10,0,0,1\n",
    "0,0,0,0,14,13,1,0,0,0,0,5,16,16,2,0,0,0,0,14,16,12,0,0,0,1,10,16,16,12,0,0,0,3,12,14,16,9,0,0,0,0,0,5,16,15,0,0,0,0,0,4,16,14,0,0,0,0,0,1,13,16,1,0,1\n",
    "0,0,0,2,16,16,2,0,0,0,0,4,16,16,2,0,0,1,4,12,16,12,0,0,0,7,16,16,16,12,0,0,0,0,3,10,16,14,0,0,0,0,0,8,16,12,0,0,0,0,0,6,16,16,2,0,0,0,0,2,12,15,4,0,1\n",
    "0,0,0,0,12,5,0,0,0,0,0,2,16,12,0,0,0,0,1,12,16,11,0,0,0,2,12,16,16,10,0,0,0,6,11,5,15,6,0,0,0,0,0,1,16,9,0,0,0,0,0,2,16,11,0,0,0,0,0,3,16,8,0,0,1\n",
    "0,0,0,1,11,9,0,0,0,0,0,7,16,13,0,0,0,0,4,14,16,9,0,0,0,10,16,11,16,8,0,0,0,0,0,3,16,6,0,0,0,0,0,3,16,8,0,0,0,0,0,5,16,10,0,0,0,0,0,2,14,6,0,0,1\n",
    "\n",
    "- next, line 250-255 (class 2)\n",
    "0,0,9,16,16,9,0,0,0,5,16,14,15,16,1,0,0,2,11,1,10,15,0,0,0,0,0,1,15,8,0,0,0,0,0,8,15,1,0,0,0,0,6,16,7,8,7,0,0,0,9,16,15,14,2,0,0,0,9,16,13,1,0,0,2\n",
    "0,0,4,14,16,5,0,0,0,4,16,16,16,8,0,0,0,10,15,9,16,4,0,0,0,1,2,13,14,0,0,0,0,0,2,16,6,0,0,0,0,0,7,16,0,5,7,0,0,0,8,16,13,16,6,0,0,0,2,15,16,6,0,0,2\n",
    "0,2,12,16,12,0,0,0,0,7,16,13,16,3,0,0,0,0,3,5,16,0,0,0,0,0,3,15,7,0,0,0,0,0,11,13,0,0,0,0,0,6,13,1,0,0,0,0,0,6,16,11,8,11,5,0,0,0,15,16,16,15,3,0,2\n",
    "0,0,7,15,15,5,0,0,0,6,16,12,16,12,0,0,0,1,7,0,16,10,0,0,0,0,0,10,15,0,0,0,0,0,1,16,7,0,0,0,0,0,10,13,1,5,1,0,0,0,12,12,13,15,3,0,0,0,10,16,13,3,0,0,2\n",
    "0,0,9,16,16,8,0,0,0,5,16,15,14,16,0,0,0,4,9,3,13,12,0,0,0,0,0,8,15,1,0,0,0,0,2,16,7,0,0,0,0,0,11,14,1,4,3,0,0,0,16,14,15,16,4,0,0,0,9,16,15,5,0,0,2\n",
    "\n",
    "- finally, line 410-415\n",
    "0,0,0,7,13,10,0,0,0,0,10,13,5,13,0,0,0,7,12,0,8,8,0,0,0,6,6,3,15,1,0,0,0,0,0,2,13,9,0,0,0,0,0,0,0,11,7,0,0,0,5,9,1,2,12,0,0,0,0,9,15,16,9,0,3\n",
    "0,0,1,7,12,3,0,0,0,4,16,12,12,10,0,0,0,14,9,0,11,8,0,0,0,7,5,0,15,4,0,0,0,0,0,2,14,7,0,0,0,0,0,0,2,13,9,0,0,0,5,10,4,0,14,5,0,0,1,9,15,16,16,8,3\n",
    "0,0,0,6,13,7,0,0,0,0,10,13,6,15,0,0,0,0,12,8,4,12,0,0,0,0,0,1,15,3,0,0,0,0,0,10,15,2,0,0,0,0,0,1,5,15,2,0,0,0,14,10,2,5,11,0,0,0,2,7,13,15,8,0,3\n",
    "0,0,0,6,15,6,0,0,0,1,11,13,8,11,0,0,0,9,13,0,9,10,0,0,0,8,9,3,15,3,0,0,0,0,0,5,14,3,0,0,0,0,3,0,5,13,2,0,0,0,9,12,5,10,7,0,0,0,0,6,12,15,5,0,3\n",
    "0,0,3,11,15,8,0,0,0,4,14,8,13,14,0,0,0,8,11,3,15,6,0,0,0,1,1,9,14,0,0,0,0,0,0,0,13,10,0,0,0,0,0,0,1,13,7,0,0,0,9,8,2,6,11,0,0,0,4,10,14,16,10,0,3\n",
    "\n",
    "This data will be used as test data, the rest of the data will be used for training\n",
    "\n",
    "See 'digits123.csv' for resulting file, and 'digits123_training.csv' for this training data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "# import X and y from csv file\n",
    "def read_logistic_data(filename):\n",
    "    my_data = genfromtxt(filename, delimiter=',')\n",
    "    X = []\n",
    "    y= []\n",
    "    for i in range(len(my_data)):\n",
    "        new_data = my_data[i]\n",
    "        y.append([new_data[-1]])\n",
    "        X.append(np.delete(new_data, -1))\n",
    "    return X, y\n",
    "\n",
    "X, temp_y = read_logistic_data('digits123.csv')\n",
    "\n",
    "'''\n",
    "for this specific handwritten digit classification problem, we need to have \n",
    "three classes: 1, 2, 3. Representation of the classes will be as follows:\n",
    "\n",
    "class '1' =   class '2' =   class '3' =\n",
    "[1,           [0,           [0,\n",
    " 0,            1,            0,\n",
    " 0]            0]            1]\n",
    "'''\n",
    "y = []\n",
    "\n",
    "for i in range(len(temp_y)):\n",
    "    if temp_y[i][0] == 1.0:\n",
    "        y.append(np.array([[1, 0, 0]]))        \n",
    "    if temp_y[i][0] == 2.0:\n",
    "        y.append(np.array([[0, 1, 0]]))      \n",
    "    if temp_y[i][0] == 3.0:\n",
    "        y.append(np.array([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chosen parameters for neural network:\n",
    "inputLayerSize = 64   # 64 features\n",
    "outputLayerSize = 3   # three classes: 1, 2, 3\n",
    "hiddenLayerSize = 35  # simple rule of thumb: n of neurons in hidden layer\n",
    "                      # is mean of inputLayerSize and outPutlayerSize.\n",
    "hiddenLayerDepth = 1\n",
    "\n",
    "# create weights for NN using above parameters\n",
    "W, W_output = weights(inputLayerSize, outputLayerSize, hiddenLayerSize, hiddenLayerDepth, -1)\n",
    "\n",
    "# sometimes running this cell results in a numpy error. To fix this: rerun the cell\n",
    "# where def weights is placed.\n",
    "# do not forget to ALSO run previous cell to obtain correct X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backpropagation(X, y, W, W_output, alpha, iterations):\n",
    "    count = 0    \n",
    "    dy_dtheta1 = 0\n",
    "    dy_dtheta2 = 0   \n",
    "    while count < iterations:\n",
    "        # loop over every training example. After loop, update weights.\n",
    "        for i in range(len(X)):\n",
    "            steps, resultY = forward(X[i], W, W_output)    \n",
    "\n",
    "            # find delta_3: hypothesis - actual answer\n",
    "            d_3 = resultY-y[i]       \n",
    "            # find delta_2: ((output weights) . d_3 ) * g'(z(2))\n",
    "            d_2 = np.dot(W_output, d_3[0].T) * sigmoid_derivative(steps[-2])\n",
    "            \n",
    "            # 3 outgoing arrows from hidden layer to final layer\n",
    "            # [hidden] --> [hidden, hidden, hidden]\n",
    "            # used for vectorization of code\n",
    "            l2 = steps[-2].reshape(35,1)\n",
    "            l2 = np.concatenate((l2, l2, l2), axis=1)        \n",
    "                       \n",
    "            # every input data point needs to connect to 35 hidden layers\n",
    "            # [input] --> [input, input, ..., input] (width = 35 elements)\n",
    "            l1 = steps[-3].reshape(64,1)\n",
    "            for i in range(5):\n",
    "                l1 = np.concatenate((l1, l1), axis = 1)    \n",
    "            l1 = np.concatenate((l1, steps[-3].reshape(64,1), steps[-3].reshape(64,1), steps[-3].reshape(64,1)), axis=1)\n",
    "            \n",
    "            # add derivative of training example i\n",
    "            dy_dtheta2 += l2*d_3\n",
    "            dy_dtheta1 += l1*d_2\n",
    "\n",
    "        # update weights\n",
    "        W_updated = list()\n",
    "        W_updated.append((1.0/len(X))*(W[0] - alpha * dy_dtheta1))\n",
    "        W_output_updated = (1.0/len(X)) * (W_output - alpha * dy_dtheta2)\n",
    "        \n",
    "        W = W_updated\n",
    "        W_output = W_output_updated\n",
    "        \n",
    "        count = count+1\n",
    "  \n",
    "    return W_updated, W_output_updated\n",
    "        \n",
    "    \n",
    "W_resulting, W_output_resulting = backpropagation(X, y, W, W_output, 0.1, 350)  \n",
    "\n",
    "# if not working make sure X == digits123.csv by running read_logistic_data again (2 cells back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99083696  0.00494433  0.00654418]\n",
      " [ 0.98963312  0.00380053  0.00930189]\n",
      " [ 0.98999883  0.00448716  0.00756343]\n",
      " [ 0.98915891  0.00355859  0.01065259]\n",
      " [ 0.99024378  0.0042243   0.0081012 ]\n",
      " [ 0.00245728  0.96420207  0.06410633]\n",
      " [ 0.01228922  0.85054736  0.05469111]\n",
      " [ 0.00431154  0.95767843  0.04307639]\n",
      " [ 0.00271788  0.93411529  0.10579419]\n",
      " [ 0.00322948  0.96509652  0.04660814]\n",
      " [ 0.00852707  0.00907672  0.98532777]\n",
      " [ 0.00514774  0.0826866   0.92067491]\n",
      " [ 0.01339865  0.01508142  0.95956003]\n",
      " [ 0.01456368  0.02298644  0.93194119]\n",
      " [ 0.00728834  0.01022931  0.98577799]]\n",
      "prediction: class 1, actual:  [1.0]\n",
      "prediction: class 1, actual:  [1.0]\n",
      "prediction: class 1, actual:  [1.0]\n",
      "prediction: class 1, actual:  [1.0]\n",
      "prediction: class 1, actual:  [1.0]\n",
      "prediction: class 2, actual:  [2.0]\n",
      "prediction: class 2, actual:  [2.0]\n",
      "prediction: class 2, actual:  [2.0]\n",
      "prediction: class 2, actual:  [2.0]\n",
      "prediction: class 2, actual:  [2.0]\n",
      "prediction: class 3, actual:  [3.0]\n",
      "prediction: class 3, actual:  [3.0]\n",
      "prediction: class 3, actual:  [3.0]\n",
      "prediction: class 3, actual:  [3.0]\n",
      "prediction: class 3, actual:  [3.0]\n"
     ]
    }
   ],
   "source": [
    "# TEST RESULTS\n",
    "\n",
    "X_test, y_test = read_logistic_data('digits123_test.csv')\n",
    "\n",
    "steps, resultY = forward(X_test, W_resulting, W_output_resulting)    \n",
    "print resultY\n",
    "\n",
    "for i in range(len(resultY)):\n",
    "    if ((resultY[i][0] > resultY[i][1]) & (resultY[i][0] > resultY[i][2])):\n",
    "        print 'prediction: class 1, actual: ', y_test[i]\n",
    "    if ((resultY[i][1] > resultY[i][0]) & (resultY[i][1] > resultY[i][2])):\n",
    "        print 'prediction: class 2, actual: ', y_test[i]\n",
    "    if ((resultY[i][2] > resultY[i][0]) & (resultY[i][2] > resultY[i][1])):\n",
    "        print 'prediction: class 3, actual: ', y_test[i]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "### QUESTION 2C ###\n",
    "\n",
    "Backpropagation for handwritten digit recognition works fully. We obtained very good results using 1 hidden layer of 35 neurons, which is approximately the mean of the number of input neurons (64) and output neurons (3). A learning rate of 0.1 is used, in combination with 350 iterations. More iterations improve the result even further, but there is no need for this because distinction between classes can be made easily already.\n",
    "\n",
    "We created a small test set (n = 15) to find out how well the neural network performed. The results can be seen in the previous cell. 15/15 training examples are correctly classified. Also, the probabilities are high, meaning the neural network is working very well."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
