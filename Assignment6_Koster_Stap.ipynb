{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren: Programming assignment 6\n",
    "\n",
    "**Student 1:**  <span style=\"color:red\">Tycho Koster</span> (<span style=\"color:red\">10667687</span>)<br>\n",
    "**Student 2:** <span style=\"color:red\">David Stap</span> (<span style=\"color:red\">10608516</span>)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import operator\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import X and y from csv file\n",
    "def read_logistic_data(filename):\n",
    "    my_data = genfromtxt(filename, delimiter=';')\n",
    "    X = []\n",
    "    y= []\n",
    "    for i in range(len(my_data)):\n",
    "        new_data = my_data[i]\n",
    "        y.append([new_data[-1]])\n",
    "        X.append(np.delete(new_data, -1))\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = read_logistic_data('digits123-1.csv')\n",
    "X_test, y_test = read_logistic_data('digits123-2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_means(k, X):\n",
    "    initialWeight = []\n",
    "    for i in range(k):\n",
    "        rand_x = int(random.uniform(0, len(X)-1))\n",
    "        initialWeight.append(X[rand_x])\n",
    "                \n",
    "    return initialWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finds best initialization of means by finding lowest cost value for 25 pairs of means.\n",
    "def best_rnd_means(k, X):\n",
    "    lowest_cost = 9999\n",
    "    for i in range(25):\n",
    "        rand_means = random_means(k, X)\n",
    "        means, clusters = KMeans(X, k, 15, rand_means)\n",
    "        cost = J(means, clusters, X)\n",
    "        \n",
    "        if cost < lowest_cost:\n",
    "            lowest_cost = cost\n",
    "            best_rnd_means = rand_means\n",
    "\n",
    "    return best_rnd_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def KMeans(X, k, iterations, means):    \n",
    "    for i in range(iterations):\n",
    "        # Initialize lists to store assigned values in clusters\n",
    "        clusters = list()\n",
    "        for i in range(len(means)):\n",
    "            clusters.append([])\n",
    "\n",
    "        # Assign all training examples to a cluster\n",
    "        for i in range(len(X)):\n",
    "            distance = 9999\n",
    "            for j in range(len(means)):\n",
    "                euclidean = math.sqrt(sum((X[i]-means[j])**2))\n",
    "                # Find the cluster with smallest euclidean distance\n",
    "                if euclidean < distance:\n",
    "                    result = j\n",
    "                    distance = euclidean\n",
    "            clusters[result].append(i)\n",
    "        \n",
    "        # Update cluster means\n",
    "        for i in range(len(clusters)):\n",
    "            mean_sum = 0\n",
    "            for x in clusters[i]:\n",
    "                mean_sum += X[x]\n",
    "            if (len(clusters[i]) > 0):\n",
    "                mean_sum = mean_sum / len(clusters[i])  \n",
    "            means[i] = mean_sum\n",
    " \n",
    "    return means, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J(means, clusters, X):\n",
    "    cost_sum = 0\n",
    "    cluster_means = []    \n",
    "    for i in range(len(clusters)):\n",
    "        for x in clusters[i]:\n",
    "            cost_sum += sum((X[x]-means[i])**2)\n",
    "    \n",
    "    return cost_sum / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost:  436.989337549\n",
      "[  0.     2.16  13.58  14.71   5.42   0.61   0.     0.     0.     8.63\n",
      "  14.95  12.87  11.24   1.61   0.     0.     0.     6.63   7.26   5.37\n",
      "  13.05   2.26   0.     0.     0.     0.34   0.61   5.45  13.16   1.76   0.\n",
      "   0.     0.     0.     0.18   9.95  11.16   0.45   0.     0.     0.     0.03\n",
      "   3.37  14.71   7.82   2.66   1.76   0.03   0.     1.24  12.92  15.68\n",
      "  13.79  13.79  10.5    0.42   0.     2.24  14.05  14.42  12.08   9.68\n",
      "   5.76   0.42]\n",
      "Number of training examples assigned to mean:  38\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     2.22  13.5   15.56   9.78   1.39   0.     0.     0.11   6.5   13.5\n",
      "  11.11  13.17   4.17   0.     0.     0.     1.94   3.     6.33  12.94\n",
      "   3.33   0.     0.     0.     0.     0.83   9.33  11.78   1.11   0.     0.\n",
      "   0.     0.     3.94  14.28   5.83   0.06   0.     0.     0.     0.78\n",
      "  10.94  12.39   1.39   0.5    0.61   0.06   0.     3.89  15.56  10.56\n",
      "   8.17   9.22   5.11   0.11   0.     2.5   14.44  15.72  15.    12.72\n",
      "   6.22   0.22]\n",
      "Number of training examples assigned to mean:  18\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     0.18   8.62  15.36  12.41   3.1    0.03   0.     0.     2.46\n",
      "  13.62  10.92  12.15   9.15   0.49   0.     0.     1.79   9.1    2.69\n",
      "   8.1   11.18   0.92   0.     0.     0.21   1.13   0.51   9.49  10.38\n",
      "   0.69   0.     0.     0.     0.03   2.08  12.31   7.15   0.28   0.     0.\n",
      "   0.     1.36   7.46  13.26   3.85   0.26   0.03   0.     0.38   8.05  15.\n",
      "  14.03   9.9    8.41   1.77   0.     0.05   9.    14.    12.59  13.08\n",
      "  12.67   5.51]\n",
      "Number of training examples assigned to mean:  39\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     0.     1.36  13.64  11.98   1.82   0.     0.     0.     0.16\n",
      "   3.75  15.23  15.66   3.55   0.     0.     0.05   1.45   5.32  15.11\n",
      "  15.23   2.84   0.     0.     0.02   1.57   5.95  14.82  14.3    1.61   0.\n",
      "   0.     0.     0.36   5.11  14.91  13.36   0.98   0.     0.     0.     0.02\n",
      "   5.09  15.14  12.57   0.82   0.     0.     0.     0.     3.82  15.14\n",
      "  12.14   1.     0.02   0.     0.     0.     1.52  12.7   13.52   2.09   0.\n",
      "   0.  ]\n",
      "Number of training examples assigned to mean:  44\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     0.     8.57  11.5    0.79   0.     0.     0.     0.     0.14\n",
      "   7.71  15.14   2.93   0.     0.     0.     0.     1.21   8.    15.57\n",
      "   6.86   0.14   0.     0.     0.     1.21   7.29  15.29  11.14   1.07   0.\n",
      "   0.     0.     0.21   0.86   8.64  13.29   3.29   0.     0.     0.     0.\n",
      "   0.     4.    12.14   6.64   0.29   0.     0.     0.     5.5   11.14\n",
      "  14.36  12.79   7.     2.79   0.     0.     7.    15.07  15.21  15.5\n",
      "  15.79  12.14]\n",
      "Number of training examples assigned to mean:  14\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     0.     0.     0.78   9.5   14.33   2.83   0.     0.     0.11\n",
      "   2.28   9.06  15.39  15.22   2.33   0.     0.     4.44  12.83  14.39\n",
      "  13.83  14.72   2.28   0.     0.     5.5   10.39   5.83  10.    14.44\n",
      "   1.44   0.     0.     1.17   2.06   0.5   10.    14.22   1.61   0.     0.\n",
      "   0.     0.     0.44  10.22  13.33   1.67   0.     0.     0.     0.     0.39\n",
      "  10.94  14.17   4.     0.     0.     0.     0.     0.5   10.78  14.22\n",
      "   3.67   0.  ]\n",
      "Number of training examples assigned to mean:  18\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     0.51   8.8   14.8   14.34   6.49   0.28   0.     0.03   5.45\n",
      "  13.11   7.49  10.75  11.48   1.22   0.     0.02   2.51   2.65   2.88\n",
      "  11.82   8.77   0.42   0.     0.     0.14   1.34  10.    14.65   6.09\n",
      "   0.11   0.     0.     0.14   0.72   4.25  10.49  13.88   3.38   0.     0.\n",
      "   0.86   2.12   0.14   0.94  11.54   9.58   0.     0.     1.46   8.54\n",
      "   4.66   4.89  13.4    8.77   0.06   0.     0.31   9.48  15.35  15.46\n",
      "  11.03   1.82   0.  ]\n",
      "Number of training examples assigned to mean:  65\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     0.63   8.53  14.61  15.53  11.29   2.11   0.03   0.     3.13\n",
      "  11.68   9.82  10.39  14.71   4.5    0.08   0.     0.71   2.34   3.66\n",
      "  11.74  11.95   1.68   0.     0.     0.03   1.87   9.74  13.08   4.18\n",
      "   0.18   0.     0.     0.     2.45   9.03  13.37   5.03   0.18   0.     0.\n",
      "   0.05   1.13   3.87  11.45  10.71   0.63   0.     0.     0.53   4.87\n",
      "   5.63  11.89  12.16   1.18   0.     0.     0.55   9.55  14.97  13.13\n",
      "   6.11   0.11   0.  ]\n",
      "Number of training examples assigned to mean:  38\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     0.04   4.77  12.    12.04   5.69   0.38   0.     0.     0.15\n",
      "   8.08  15.54  15.38  10.77   0.88   0.     0.     0.5    9.27  15.92\n",
      "  15.81   9.65   0.42   0.     0.     1.08  10.42  15.92  15.23   7.73\n",
      "   0.15   0.     0.     1.19  11.31  15.42  15.     5.77   0.04   0.     0.\n",
      "   1.23  12.31  15.69  14.81   4.69   0.     0.     0.     0.54  11.08\n",
      "  15.73  14.92   5.96   0.23   0.     0.     0.04   4.81  11.35  12.58\n",
      "   6.54   0.58   0.  ]\n",
      "Number of training examples assigned to mean:  26\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example run: random means initialization (pick random mean that has lowest cost)\n",
    "means = best_rnd_means(3, X_train)\n",
    "means, clusters = KMeans(X_train, k, 15, rand_means)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "print 'Total cost: ', J(means, clusters, X_train)\n",
    "for i in range(len(means)):\n",
    "    print means[i]\n",
    "    print 'Number of training examples assigned to mean: ', len(clusters[i])\n",
    "    print '-------------------------------------------------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost:  698.907441236\n",
      "[  0.     0.09   3.29  12.9   12.44   4.34   0.55   0.01   0.     0.4\n",
      "   5.78  14.51  15.04   7.26   0.79   0.01   0.03   1.07   6.31  14.29\n",
      "  15.12   6.19   0.25   0.     0.01   1.51   7.25  14.88  14.53   3.81\n",
      "   0.05   0.     0.     0.68   6.91  14.79  13.64   2.65   0.01   0.     0.\n",
      "   0.41   7.04  14.55  13.66   2.45   0.     0.     0.     0.21   5.9\n",
      "  14.25  13.41   3.17   0.09   0.     0.     0.05   3.58  12.19  12.9\n",
      "   3.67   0.19   0.  ]\n",
      "Number of training examples assigned to mean:  80\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     1.36  11.82  14.56   7.16   0.96   0.     0.     0.02   5.56\n",
      "  13.74  12.26  10.99   3.23   0.03   0.     0.     3.79   7.6    6.24\n",
      "  11.51   3.98   0.03   0.     0.     0.35   1.83   6.28  12.36   3.17   0.\n",
      "   0.     0.     0.03   0.96   8.71  11.27   1.73   0.     0.     0.     0.16\n",
      "   3.78  11.3    8.67   2.52   0.88   0.02   0.     1.37  11.45  14.06\n",
      "  12.71  11.54   8.49   1.01   0.     1.4   12.01  14.45  13.14  12.12\n",
      "   9.23   3.43]\n",
      "Number of training examples assigned to mean:  94\n",
      "-------------------------------------------------------------------------\n",
      "[  0.     0.41   7.31  13.02  13.83   8.48   0.92   0.     0.02   3.76\n",
      "  11.17   8.82  11.13  12.88   2.13   0.02   0.01   2.21   4.33   4.56\n",
      "  11.06  10.95   1.24   0.     0.     0.71   2.46   8.1   12.44   7.87\n",
      "   0.53   0.     0.     0.18   1.14   4.13  11.06  11.98   2.12   0.     0.\n",
      "   0.46   1.47   1.12   5.71  11.71   5.44   0.01   0.     0.91   6.43\n",
      "   5.27   8.25  13.52   6.4    0.28   0.     0.3    7.93  13.37  14.17\n",
      "  10.78   2.9    0.66]\n",
      "Number of training examples assigned to mean:  126\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example run: handselected values for mean.\n",
    "means, clusters = KMeans(X_train, 3, 15, [X_train[0], X_train[150], X_train[299]])\n",
    "\n",
    "print 'Total cost: ', J(means, clusters, X_train)\n",
    "for i in range(len(means)):\n",
    "    print means[i]\n",
    "    print 'Number of training examples assigned to mean: ', len(clusters[i])\n",
    "    print '-------------------------------------------------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: optimized k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x93f34e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost = []\n",
    "k_list = []\n",
    "rand_means = random_means(9, X_train)\n",
    "for k in range(1,9):\n",
    "    means, clusters = KMeans(X_train, k, 15, rand_means[0:k])\n",
    "    cost.append(J(means, clusters, X_train))\n",
    "    k_list.append(k)\n",
    "    \n",
    "plt.plot(k_list, cost)\n",
    "plt.ylabel(\"cost\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the elbow method, it becomes clear that 3 is the optimal number for k (as suspected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# run random mean initialization x times, pick one that results in lowest cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: compare cluster labels to given labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified Cluster 1 :\n",
      "[238, 277, 278, 279, 280, 281, 290, 292]\n",
      "Classified Cluster 2 :\n",
      "[3, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "Classified Cluster 3 :\n",
      "[14, 50, 52, 55, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 108, 110, 130, 150, 151, 152, 153, 154, 156, 158, 175, 177, 179, 181, 184]\n"
     ]
    }
   ],
   "source": [
    "means, clusters = KMeans(X_train, 3, 15, [X_train[0], X_train[150], X_train[299]])\n",
    "wrong = []\n",
    "for i in range(len(clusters)):\n",
    "    wrong.append([])\n",
    "    for x in clusters[i]:\n",
    "        if y_train[x][0] != i+1:\n",
    "            wrong[i].append(x)\n",
    "\n",
    "\n",
    "            \n",
    "for i in range(len(clusters)):\n",
    "    print \"Classified Cluster\", i+1,\":\"\n",
    "    print wrong[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Discussion\n",
    "\n",
    "The data shown above are all the errors that were made by the k-means classifier. It shows the label that was given by the dataset below the cluster that it was classified under.\n",
    "\n",
    "It is noticable that cluster 3 has the most errors with classifying the digits data. Cluster 1 only has errors with data that was originally from cluster 3. Cluster 2 has the same problem with cluster 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 4: Anomalous detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrongly classified euclidean distance to means sorted:\n",
      "[42.16283492041394, 41.63949454146789, 41.32553552733848, 40.987104603328255, 40.89405453296512, 40.22977513305581, 40.016741111202414, 38.52362558081932, 38.176780289699416, 38.145792348867296, 37.724983432733275, 37.68591856449683, 37.57477050741371, 37.34400052217224, 37.21887063125048, 37.05795393857274, 37.05388458463959, 37.02259988836996, 36.784454382265835, 36.240852294061746, 36.00428933000126, 35.85096860774306, 35.70167237582629, 35.30942733419902, 35.28169797303653, 35.25674377845195, 35.01731835869237, 34.7977487525206, 34.185393905330166, 33.57469588963737, 33.55814915933238, 33.215360965698274, 33.06848008300352, 33.03494851107311, 32.9856613527042, 32.96984952677244, 32.80616300431612, 32.7696125346171, 32.7643150320611, 32.47962240290116, 32.475036552629355, 32.35011662036899, 32.163617634210986, 32.110736132950926, 31.913285420474267, 31.879265574393298, 31.67929393782464, 31.483024616486063, 30.69200832464373, 30.68006350982594, 30.372395794600315, 29.97322065680332, 28.42005233985328]\n",
      "\n",
      "Correctly classified distance to means sorted:\n",
      "[37.96100607016904, 37.95358184677699, 36.86766028648957, 35.9018487513411, 35.06528732236484, 34.72600070144204, 34.54520683383939, 34.19166167198016, 34.12508778180489, 34.054873103174764, 33.613737046683, 33.539597416178, 33.31866173715037, 32.700288227452106, 32.52577241873033, 32.36050783253748, 31.949462289521694, 31.535684153035273, 31.53165108856827, 31.09621343179774, 31.048194195666056, 30.8187964051254, 30.56194127364559, 30.53241651179753, 30.32501809560661, 30.093300292571236, 29.877236747904877, 29.734111064512096, 29.624068692587176, 29.594989080667137, 29.533445024243278, 29.51541824179746, 29.458859024069483, 29.421070255855753, 29.360753649647403, 29.32666927916476, 29.29038870940943, 29.070592959208795, 29.040879462331652, 28.746728074687038, 28.692234682327545, 28.571390848189385, 28.43720054787391, 28.415427319203047, 28.39891369964224, 28.258501246733424, 28.20423838582422, 28.089537355455633, 27.893901266964367, 27.834086917982834, 27.811552029756538, 27.688858815312898, 27.309046369389065, 27.240659518622277, 27.044171057893728, 27.03918169876403, 27.000115478043135, 26.89654949988939, 26.71421486194864, 26.654725190855, 26.614649521663775, 26.2891110689719, 26.28668296347549, 26.221444836381135, 26.191811274098953, 26.181416181864225, 26.12952865196187, 26.004393052903318, 25.99325633698095, 25.965298227021016, 25.920327518856844, 25.884829051009785, 25.84428120007015, 25.635960430218315, 25.453376495074284, 25.4207117334274, 25.329383419244397, 25.32374281407428, 25.323656401923838, 25.226428204279667, 25.123312659803993, 25.12015343988841, 25.116217370456084, 25.087301169031253, 25.020479112119336, 24.989646382129038, 24.976378609724733, 24.872661210460727, 24.809763703026274, 24.805719020925892, 24.793558065391707, 24.787829871388716, 24.771945335204453, 24.76722094093655, 24.762358025842367, 24.760338749702107, 24.710833321576022, 24.6384629162471, 24.583126870969597, 24.572985831051568, 24.522549667130267, 24.40723469542569, 24.406798824955455, 24.363775105984008, 24.335092192513684, 24.251569792334784, 24.21820835891643, 24.17952182509732, 24.12842388143634, 24.093532313044673, 24.08474221587707, 24.034857499057487, 23.998145714459405, 23.939869690471177, 23.83903049622614, 23.788216656586098, 23.780230760024175, 23.774867635083893, 23.678932173597843, 23.604011841210383, 23.53868577238663, 23.527944434382256, 23.489932636447165, 23.435003200341153, 23.404046124548632, 23.33278667969951, 23.30658087361037, 23.291933632388723, 23.273867372964276, 23.261587949297496, 23.20881892062493, 23.15145138478952, 23.05734495692392, 22.993762316715195, 22.79965734391638, 22.736723967626588, 22.666591605267868, 22.65559703688044, 22.59707943727713, 22.56433856774889, 22.547443052564002, 22.52314254524396, 22.451850723426553, 22.444918689984156, 22.42079010387116, 22.367916669755473, 22.361876732723186, 22.354784601016025, 22.33807738419474, 22.33027443769075, 22.273692123146997, 22.2727271567718, 22.253015796025053, 22.10774225409949, 22.020430854095476, 21.9774028453697, 21.869300330988036, 21.823526226688333, 21.749698273769226, 21.70136804443443, 21.671938134240115, 21.627659574468083, 21.591722191148143, 21.587545225049038, 21.582397610583378, 21.44588238881857, 21.364675869294157, 21.292145081622724, 21.265552985414324, 21.25819836515847, 21.112094200764407, 21.042810806099908, 21.029606060641264, 21.008365080365774, 20.8400779787035, 20.806234041748162, 20.770244611570895, 20.73046713351381, 20.686333048657996, 20.61221430814281, 20.472875650296057, 20.354942793953626, 20.348703346679276, 20.130185667300736, 20.09040504818158, 20.083315691414047, 20.020598767269675, 19.907897302327033, 19.86310380357597, 19.763840983747038, 19.725252873081995, 19.724968314296476, 19.699074369358236, 19.641789292763406, 19.58311260550089, 19.540918751914766, 19.532387778651152, 19.41950979805465, 19.342113658975865, 19.319662911137968, 19.29420371424935, 19.290895194170883, 18.99468904353452, 18.994315297094605, 18.788942892030942, 18.65220080417573, 18.569642771282954, 18.420740922805596, 18.39155716626518, 18.231674540245102, 18.212408651297256, 18.200137921763357, 18.143351139852157, 18.113422763271327, 18.091211958113217, 17.858968160679726, 17.666334509456114, 17.657133827436436, 17.520684204676485, 17.27062173171539, 17.16681027448023, 16.986639407703542, 16.862091411037582, 16.801415820183074, 16.6883005425957, 16.610376119741954, 16.54839493727413, 16.546884147778396, 16.23266382945202, 16.040186207232697, 15.578811732606566, 15.478190301194774, 15.476575041009557, 15.435328794683965, 15.189614050396408, 14.78256997277537, 14.728352759219204, 14.628717476251975, 14.214407303858998, 13.660138176460734, 13.600528482378913, 13.411725280514808, 13.054668705103166, 12.832551383103828, 12.376363561240433, 11.25163876953042, 10.976081951224671]\n",
      "\n",
      "Index of the anomalous datapoints sorted:\n",
      "[3, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 38, 48, 49, 50, 52, 55, 57, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 103, 106, 107, 108, 109, 110, 130, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 156, 158, 175, 176, 177, 179, 181, 184, 192, 212, 216, 217, 221, 232, 238, 277, 279, 280, 281, 288, 289, 290, 292, 293, 294, 298]\n"
     ]
    }
   ],
   "source": [
    "means, clusters = KMeans(X_train, 3, 15, [X_train[0], X_train[150], X_train[299]])\n",
    "wrong_data = []\n",
    "anomalous_sets = []\n",
    "normal_data = []\n",
    "for i in range(len(clusters)):\n",
    "    for x in clusters[i]:\n",
    "        euclidean = math.sqrt(sum((X_train[x]-means[i])**2))\n",
    "        if euclidean > 30:\n",
    "            anomalous_sets.append(x)\n",
    "        if y_train[x][0] != i+1:\n",
    "            wrong_data.append(euclidean)\n",
    "        else:\n",
    "            normal_data.append(euclidean)\n",
    "        \n",
    "       \n",
    "\n",
    "print \"Wrongly classified euclidean distance to means sorted:\"\n",
    "print sorted(wrong_data, reverse=True)\n",
    "print\n",
    "print \"Correctly classified distance to means sorted:\"\n",
    "print sorted(normal_data, reverse=True)\n",
    "print\n",
    "print \"Index of the anomalous datapoints sorted:\"\n",
    "print sorted(anomalous_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "\n",
    "We created two lists with distances, one for the instances that are in the wrong cluster (i.e. cluster label is not most frequent class in cluster) and one for the instances that are in the correct cluster. As can be seen above, all instances in the wrong cluster have a relatively high distance to the cluster.\n",
    "After inspecting the data, we came up with the following definition: Anomalous iff distance > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Anomalies inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A normal cluster 1:\n",
      "[  0.   0.   4.  10.  11.   4.   0.   0.   0.   1.  11.  16.  16.  14.   0.\n",
      "   0.   0.   4.  16.  16.  16.  12.   0.   0.   0.   4.  16.  16.  16.   7.\n",
      "   0.   0.   0.   4.  16.  16.  16.   8.   0.   0.   0.   4.  16.  16.  16.\n",
      "   7.   0.   0.   0.   3.  15.  16.  16.  12.   0.   0.   0.   0.   5.  12.\n",
      "  12.  12.   1.   0.]\n",
      "An anomaly cluster 1:\n",
      "[  0.   0.   0.   0.  13.  12.   0.   0.   0.   0.   0.  10.  16.  14.   0.\n",
      "   0.   0.   1.  12.  16.  16.  11.   0.   0.   0.  11.  16.  12.  16.   8.\n",
      "   0.   0.   0.   6.   4.   7.  16.   6.   0.   0.   0.   0.   0.   6.  16.\n",
      "   5.   0.   0.   0.   0.   0.   4.  16.   8.   0.   0.   0.   0.   0.   0.\n",
      "  15.  11.   0.   0.]\n",
      "A normal cluster 2:\n",
      "[  0.   0.   2.  16.  10.   0.   0.   0.   0.   0.   3.  16.  16.   1.   0.\n",
      "   0.   0.   0.   5.  16.  14.   0.   0.   0.   0.   0.   3.  16.  13.   0.\n",
      "   0.   0.   0.   0.   1.  16.  15.   0.   0.   0.   0.   0.   1.  16.  16.\n",
      "   0.   0.   0.   0.   0.   2.  16.  15.   2.   0.   0.   0.   0.   0.  15.\n",
      "  16.  11.   0.   0.]\n",
      "An anomaly cluster 2:\n",
      "[  0.   0.  14.  10.   0.   0.   0.   0.   0.   0.  15.  13.   0.   0.   0.\n",
      "   0.   0.  11.  16.  16.   2.   0.   0.   0.   0.   3.  10.  16.   5.   0.\n",
      "   0.   0.   0.   0.   0.  14.  10.   0.   0.   0.   0.   0.   0.  10.  14.\n",
      "   0.   0.   0.   0.   0.   9.  14.  16.  11.   6.   0.   0.   0.  12.  16.\n",
      "  16.  16.  16.   9.]\n",
      "A normal cluster 3:\n",
      "[  0.   1.   8.  14.  15.   2.   0.   0.   0.   2.  13.   9.  14.   8.   0.\n",
      "   0.   0.   0.   0.   0.  12.   9.   0.   0.   0.   0.   2.  13.  13.   0.\n",
      "   0.   0.   0.   0.   3.  15.  16.   6.   0.   0.   0.   1.   1.   0.  12.\n",
      "  14.   0.   0.   0.   5.  13.   5.   6.  16.   1.   0.   0.   1.   9.  12.\n",
      "  13.   9.   0.   0.]\n",
      "An anomaly cluster 3:\n",
      "[  0.   0.   2.  10.  12.   2.   0.   0.   0.   0.  11.  16.  13.  10.   0.\n",
      "   0.   0.   0.   9.   7.   2.  15.   0.   0.   0.   0.   3.   2.   3.  15.\n",
      "   0.   0.   0.   0.   0.   0.  10.  10.   0.   0.   0.   0.   2.   7.  16.\n",
      "   7.   0.   0.   0.   0.   9.  16.  16.  16.  16.   3.   0.   0.   3.  13.\n",
      "   9.   8.   4.   0.]\n"
     ]
    }
   ],
   "source": [
    "print \"A normal cluster 1:\"\n",
    "print X_train[1]\n",
    "print \"An anomaly cluster 1:\"\n",
    "print X_train[38]\n",
    "print \"A normal cluster 2:\"\n",
    "print X_train[60]\n",
    "print \"An anomaly cluster 2:\"\n",
    "print X_train[3]\n",
    "print \"A normal cluster 3:\"\n",
    "print X_train[200]\n",
    "print \"An anomaly cluster 3:\"\n",
    "print X_train[175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "As we can see from the datapoints that are presented above. There is a lot of big differences between the datapoints that are considered normal and that are consideren anomaly so they do make sense, because they dont really look like eachother. Another thing we noticed is that datapoints in cluster 2 and 3 look relativaly similar, this is also why the classifier makes a lot of errors in seperating these two classes. A lot of classify errors come from 2 being classified as 3 and vice versa."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
