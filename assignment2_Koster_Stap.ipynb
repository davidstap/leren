{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren: Programming assignment 2\n",
    "** This assignment can be done in teams of 2 **\n",
    "\n",
    "**Student 1:**  <span style=\"color:red\">Tycho Koster</span> (<span style=\"color:red\">10667687</span>)<br>\n",
    "**Student 2:** <span style=\"color:red\">David Stap</span> (<span style=\"color:red\">10608516</span>)<br>\n",
    "\n",
    "-----------------------------------\n",
    "This notebook provides a template for your programming assignment 2. You may want to use parts of your code from the previous assignment(s) as a starting point for this assignment. \n",
    "\n",
    "The code you hand-in should follow the structure from this document. Write down your functions in the cells they belong to. Note that the structure corresponds with the structure from the actual programming assignment. Make sure you read this for the full explanation of what is expected of you. \n",
    "\n",
    "**Submission:**\n",
    "\n",
    "* Make sure your code can be run from top to bottom without errors.\n",
    "* Include your data files in the zip file.\n",
    "* Comment your code\n",
    "\n",
    "One way be sure you code can be run without errors is by quiting iPython completely and then restart iPython and run all cells again (you can do this by going to the menu bar above: Cell > Run all). This way you make sure that no old definitions of functions or values of variables are left (that your program might still be using).\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "If you have any questions ask your teaching assistent. We are here for you.\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "1) Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yvonn\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "get_training_data\n",
    "Input: feature (name of feature to retrieve from file).\n",
    "Output: list of feature values.\n",
    "'''\n",
    "def get_training_data(feature):\n",
    "    check = ['MLS','Bedrooms','Bathrooms','Size','Price']\n",
    "    index = check.index(feature)\n",
    "    training_list = list()\n",
    "    \n",
    "    \n",
    "    with open('housesRegr.csv', 'rU') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            training_data = row[0].split(';')\n",
    "            training_list.append(training_data[index])\n",
    "\n",
    "    return training_list[1:];\n",
    "\n",
    "\n",
    "prices = np.array(get_training_data('Price')).astype(np.float)\n",
    "bedrooms = np.array(get_training_data('Bedrooms')).astype(np.float)\n",
    "bathrooms = np.array(get_training_data('Bathrooms')).astype(np.float)\n",
    "size = np.array(get_training_data('Size')).astype(np.float)\n",
    "\n",
    "\n",
    "#Apply feature scaling, because the range of size is way bigger then that of the rest.\n",
    "bedrooms = np.divide(bedrooms, np.amax(bedrooms))\n",
    "prices = np.divide(prices, np.amax(prices))\n",
    "bathrooms = np.divide(bathrooms, np.amax(bathrooms))\n",
    "size = np.divide(size, np.amax(size))\n",
    "#Store all features into a list, to use for multivariate liniear regression.\n",
    "allfeatures = [bedrooms, bathrooms, size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "'''\n",
    "gradient_function_multiple\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values).\n",
    "Output: gradient for every theta in array with multivariate liniear regression.\n",
    "'''\n",
    "def gradient_function_multiple(alpha, theta_array, theta, x, y):\n",
    "    m = len(x[0])\n",
    "    totalsum = 0\n",
    "    for i in range(len(x[0])):\n",
    "        temp_x = []\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        if theta == 0:\n",
    "            totalsum += (Hx_multiple(theta_array, temp_x) - y[i])\n",
    "        else:\n",
    "            totalsum += (Hx_multiple(theta_array, temp_x) - y[i])*x[theta-1][i]\n",
    "    return (alpha*totalsum)/m\n",
    "    \n",
    "'''\n",
    "Hx_multiple\n",
    "Input: theta_array (list of theta values), x (list of values).\n",
    "Output: prediction of y with multivariate hypotheses calculations.\n",
    "'''\n",
    "def Hx_multiple(theta_array, x):\n",
    "    hx = theta_array[0]\n",
    "    for i in range(1, len(theta_array)):\n",
    "        hx += theta_array[i]*x[i-1]\n",
    "    return hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Parameter updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "\n",
    "'''\n",
    "plot_cost_vs_iter\n",
    "Input: cost (array of all the cost for different theta values).\n",
    "Output: Plot with cost on y axis and iterations on x axis.\n",
    "'''\n",
    "def plot_cost_vs_iter(cost):\n",
    "    plt.plot(cost)\n",
    "    plt.ylabel(r\"J(${\\Theta}$)\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "update_theta_multiple\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values),\n",
    "iterations (number of iterations to be done).\n",
    "Output: All values of theta after the amount of iterations with the use of multivariate liniear regression.\n",
    "'''\n",
    "def update_theta_multiple(alpha, theta_array, x, y, iterations):\n",
    "    cost = []\n",
    "    cost.append(cost_function_multiple(theta_array, x, y))\n",
    "    for i in range(iterations):\n",
    "        for j in range(len(theta_array)):\n",
    "            theta_array[j] -= gradient_function_multiple(alpha, theta_array, j, x, y)\n",
    "        cost.append(cost_function_multiple(theta_array, x, y))\n",
    "    plot_cost_vs_iter(cost)\n",
    "    return theta_array\n",
    "        \n",
    "#print(update_theta_multiple(0.5, [0.2,0.2,0.2], allfeatures, prices, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "cost_function_multiple\n",
    "Input: theta_array (list of theta values), x (list of multiple features with values), y (list of values).\n",
    "Output: The cost of the theta values with multivariate liniear regression.\n",
    "'''\n",
    "def cost_function_multiple(theta_array, x, y):\n",
    "    m = len(x[0])\n",
    "    totalsum = 0\n",
    "    for i in range(len(x[0])):\n",
    "        temp_x = []\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        totalsum += (Hx_multiple(theta_array, temp_x) - y[i])**2\n",
    "    return totalsum/(2*m)\n",
    "\n",
    "#print(cost_function_multiple([0.2,0.2,0.2], [[2,4,4], [3,5,3]], [6,6,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Optimization learning rate and iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "multiple\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values),\n",
    "iterations (number of iterations to be done).\n",
    "Output: Plot of the cost vs iterations and all theta values after iterations amount.'''\n",
    "def multiple(alpha, theta_array, x, y, iterations):\n",
    "    update_theta_multiple(alpha, theta_array, x, y, iterations)\n",
    "\n",
    "    \n",
    "multiple(0.01, [0.2,0.2,0.2], allfeatures, prices, 10)\n",
    "multiple(0.1, [0.2,0.2,0.2], allfeatures, prices, 10)\n",
    "multiple(0.3, [0.2,0.2,0.2], allfeatures, prices, 10)\n",
    "multiple(0.6, [0.2,0.2,0.2], allfeatures, prices, 10)\n",
    "multiple(0.8, [0.2,0.2,0.2], allfeatures, prices, 10)\n",
    "multiple(1, [0.2,0.2,0.2], allfeatures, prices, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "To optimize the learning rate and iterations we plotted out alpha against the iterations and kept changing alpha until we were\n",
    "satisfied with our learning rate and number of iterations. As you can see it the previous tab. Alpha rate of 1 makes the cost really small on the first iteration, but the alpha rate 0.8 is eventually smaller after more iterations. The difference between the cost after more iterations is very small. We chose to go for alpha rate 0.8 and 10 iterations. \n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "1) Extension to polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "gradient_function_polynomial\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values).\n",
    "Output: gradient for every theta in array with polynomial multivariate liniear regression.\n",
    "'''\n",
    "def gradient_function_polynomial(alpha, theta_array, theta, x, y):\n",
    "    m = len(x[0])\n",
    "    totalsum = 0\n",
    "    for i in range(len(x[0])):\n",
    "        temp_x = []\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        if theta == 0:\n",
    "            totalsum += (Hx_polynomial(theta_array, temp_x) - y[i])\n",
    "        else:\n",
    "            totalsum += (Hx_polynomial(theta_array, temp_x) - y[i])*x[theta-1][i]\n",
    "    return (alpha*totalsum)/m\n",
    "    \n",
    "'''\n",
    "Hx_polynomial\n",
    "Input: theta_array (list of theta values), x (list of values).\n",
    "Output: prediction of y with polynomial multivariate hypotheses calculations.\n",
    "''' \n",
    "def Hx_polynomial(theta_array, x):\n",
    "    hx = theta_array[0]\n",
    "    for i in range(1, len(theta_array)):\n",
    "        hx += theta_array[i]*(x[i-1]**2)\n",
    "    return hx\n",
    "\n",
    "'''\n",
    "update_theta_polynomial\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values),\n",
    "iterations (number of iterations to be done).\n",
    "Output: All values of theta after the amount of iterations with the use of polynomial multivariate liniear regression.\n",
    "'''\n",
    "def update_theta_polynomial(alpha, theta_array, x, y, iterations):\n",
    "    cost = []\n",
    "    cost.append(cost_function_polynomial(theta_array, x, y))\n",
    "    for i in range(iterations):\n",
    "        for j in range(len(theta_array)):\n",
    "            theta_array[j] -= gradient_function_polynomial(alpha, theta_array, j, x, y)\n",
    "        cost.append(cost_function_polynomial(theta_array, x, y))\n",
    "    plot_cost_vs_iter(cost)\n",
    "    return theta_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "cost_function_polynomial\n",
    "Input: theta_array (list of theta values), x (list of multiple features with values), y (list of values).\n",
    "Output: The cost of the theta values with polynomial multivariate liniear regression.\n",
    "'''\n",
    "def cost_function_polynomial(theta_array, x, y):\n",
    "    m = len(x[0])\n",
    "    totalsum = 0\n",
    "    for i in range(len(x[0])):\n",
    "        temp_x = []\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        totalsum += (Hx_polynomial(theta_array, temp_x) - y[i])**2\n",
    "    return totalsum/(2*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Optimization learning rate and iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03772216230550398, 0.19852658112610574, 0.21199701053835199]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_theta_polynomial(0.01, [0.2, 0.2, 0.2], allfeatures, prices, 10)\n",
    "update_theta_polynomial(0.03, [0.2, 0.2, 0.2], allfeatures, prices, 10)\n",
    "update_theta_polynomial(0.1, [0.2, 0.2, 0.2], allfeatures, prices, 10)\n",
    "update_theta_polynomial(0.3, [0.2, 0.2, 0.2], allfeatures, prices, 10)\n",
    "update_theta_polynomial(0.8, [0.2, 0.2, 0.2], allfeatures, prices, 10)\n",
    "update_theta_polynomial(1, [0.2, 0.2, 0.2], allfeatures, prices, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "If we look at the graphs that are plotted from the previous block, it is noticable that the it is the same case as the multivariate regression without the polynomial components. So for this we also picked 0.8 as a learning rate and 10 iterations.\n",
    "Eventually the cost function is the same between polynomial and the version without squared variables.\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "1) Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542\n",
      "542\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "def read_logistic_data():\n",
    "    my_data = genfromtxt('digits123.csv', delimiter=',')\n",
    "    x = []\n",
    "    y= []\n",
    "    for i in range(len(my_data)):\n",
    "        new_data = my_data[i]\n",
    "        y.append(new_data[-1])\n",
    "        x.append(np.delete(new_data, -1))\n",
    "    return x, y\n",
    "x, y = read_logistic_data()\n",
    "def create_theta_array(x):\n",
    "    theta = [0.5]\n",
    "    for i in range(len(x)):\n",
    "        theta.append(0.5)\n",
    "    return theta\n",
    "theta_array = create_theta_array(x[0])\n",
    "print(len(y))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Gradient calculating and parameter updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "gradient_function_logistic\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values)\n",
    "Output: gradient for every theta in array\n",
    "'''\n",
    "\n",
    "def gradient_function_logistic(alpha, theta_array, theta, x, y):\n",
    "    m = len(x)\n",
    "    totalsum = 0\n",
    "    for i in range(0, len(x[0])):\n",
    "        temp_x = []\n",
    "        #temporary x for the values of all features at that specific index\n",
    "        for j in range(len(x)):\n",
    "            temp_x.append(x[j][i])\n",
    "        if theta == 0:\n",
    "            totalsum += (Hx_logistic(theta_array, temp_x) - y[i])\n",
    "        else:\n",
    "            totalsum += (Hx_logistic(theta_array, temp_x) - y[i])*x[theta-1][i]\n",
    "    return (alpha*totalsum)/m\n",
    "    \n",
    "'''\n",
    "Hx_logistic\n",
    "Input: theta_array (list of theta values), x (list of values)\n",
    "Output: prediction of y with logistic hypotheses calculations\n",
    "'''\n",
    "def Hx_logistic(theta_array, x):\n",
    "    exponent = theta_array[0]\n",
    "    for i in range(1, len(theta_array)):\n",
    "        exponent += theta_array[i]*x[i-1]\n",
    "    e = -exponent\n",
    "    return 1.0/(1 + math.exp(e))\n",
    "\n",
    "'''\n",
    "update_theta_logistic\n",
    "Input: alpha (learning rate), theta_array (list of theta values), x (list of multiple features with values), y (list of values),\n",
    "iterations (number of iterations to be done)\n",
    "Output: All values of theta after the amount of iterations with the use of logistic regression.\n",
    "'''\n",
    "def update_theta_logistic(alpha, theta_array, x, y, iterations):\n",
    "    cost = []\n",
    "    cost.append(cost_function_logistic(theta_array, x, y))\n",
    "    for i in range(iterations):\n",
    "        for j in range(len(theta_array)):\n",
    "            theta_array[j] -= gradient_function_logistic(alpha, theta_array,j, x, y)\n",
    "        cost.append(cost_function_logistic(theta_array, x, y))\n",
    "    return theta_array, cost\n",
    "\n",
    "# print(update_theta_logistic(0.01,theta_array, x, y, 1))\n",
    "# theta_array = update_theta_logistic(0.5, [0.5,0.5,0.5], [[5,5,3,2],[3,5,3,4]], [0,0,1,1], 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0873785097662279"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cost_function_logistic\n",
    "Input: theta_array (list of theta values), x (list of multiple features with values), y (list of values)\n",
    "Output: The cost of the theta values with logistic regression.\n",
    "'''\n",
    "def cost_function_logistic(theta_array, x, y):\n",
    "    m = len(x[0])\n",
    "    totalsum = 0\n",
    "    for i in range(len(x)):\n",
    "        temp_x = []\n",
    "        for j in range(len(x[0])):\n",
    "            temp_x.append(x[i][j])\n",
    "        if(Hx_logistic(theta_array, temp_x) == 1):\n",
    "            totalsum += math.log10(Hx_logistic(theta_array, temp_x))\n",
    "        else:\n",
    "            totalsum += y[i]*math.log10(Hx_logistic(theta_array, temp_x)) + (1 - y[])*math.log10(1 - Hx_logistic(theta_array, temp_x))\n",
    "    return totalsum/m\n",
    "\n",
    "cost_function_logistic([0.5,0.5,0.5], [[5,5,3,2],[3,5,3,4]], [0,0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Pairwise comparison of classess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Optimization learning rate and iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logreg(alpha, theta_array, x, y, iterations):\n",
    "    theta_array, cost = update_theta_logistic(alpha, theta_array, x, y, iterations)\n",
    "    plot_cost_vs_iter(cost)\n",
    "logreg(0.01, theta_array, x, y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "We think we did not quite understand how the digits in the csv are represented. Our cost is always 0 because we have a lot of features per class. This means that 1/1+e^h(x) always equals 1, which also means that the cost will be equal to log(1), which is 0. This is a problem we did not know how to fix.\n",
    "\n",
    "-----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
