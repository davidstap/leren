{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren: Programming assignment 4\n",
    "\n",
    "**Student 1:**  <span style=\"color:red\">Tycho Koster</span> (<span style=\"color:red\">10667687</span>)<br>\n",
    "**Student 2:** <span style=\"color:red\">David Stap</span> (<span style=\"color:red\">10608516</span>)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read logistic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "\n",
    "# import X and y from csv file\n",
    "def read_logistic_data(filename):\n",
    "    my_data = genfromtxt(filename, delimiter=';')\n",
    "    X = []\n",
    "    y= []\n",
    "    for i in range(len(my_data)):\n",
    "        new_data = my_data[i]\n",
    "        y.append([new_data[-1]])\n",
    "        X.append(np.delete(new_data, -1))\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = read_logistic_data('digits123-1.csv')\n",
    "X_test, y_test = read_logistic_data('digits123-2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1): k NN\n",
    "For both unweighted k-NN and weighted k-NN the same function is used. User can choose between these options by giving argument weight=False for unweighted and weight=True for weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "INPUT: \n",
    "- n-dimensional vector x (numpy array)\n",
    "- n-dimensional vector y (numpy array)\n",
    "\n",
    "OUTPUT: \n",
    "- euclidean distance of vectors x and y\n",
    "'''\n",
    "def distance(x, y):\n",
    "    return math.sqrt(sum((x-y)**2))\n",
    "\n",
    "'''\n",
    "INPUT:\n",
    "- training set X (training examples with features)\n",
    "- training set y (classes corresponding to features)\n",
    "- test_instance (to be classified instance)\n",
    "- k (determines how many neighbors are taken into account)\n",
    "- weight (can be True / False. Determines if neighbors are weighted or not)\n",
    "\n",
    "OUTPUT:\n",
    "- class assignment for test instance\n",
    "'''\n",
    "def k_nearest_neighbors(training_set_x, training_set_y, test_instance, k, weight=False):    \n",
    "    distances = []  \n",
    "    result = 0\n",
    "    # find all distances from test_instance to training_set\n",
    "    for i in range(len(training_set_x)):\n",
    "        distances.append((i, distance(training_set_x[i], test_instance)))\n",
    "    # sort list: ascending distances\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "\n",
    "    prediction = 0    \n",
    "    # calculate non-weighed average\n",
    "    if weight == False:\n",
    "        for i in range(k):\n",
    "            prediction += training_set_y[distances[:k][i][0]][0]\n",
    "        \n",
    "        result = round(prediction/k)\n",
    "        \n",
    "    # calculate weighed average\n",
    "    if weight == True:\n",
    "        # create weights out of normalized values for 1/distance\n",
    "        weightlist = []\n",
    "        for i in range(k):\n",
    "            weightlist.append(1/distances[:k][i][1])\n",
    "        normalizer = 1/sum(weightlist)        \n",
    "        for i in range(k):\n",
    "            weightlist[i] = weightlist[i] * normalizer\n",
    "            \n",
    "        # multiply with weight\n",
    "        for i in range(k):\n",
    "            prediction += weightlist[i] * training_set_y[distances[:k][i][0]][0]\n",
    "        \n",
    "        result = round(prediction)\n",
    "                       \n",
    "    return result\n",
    "    \n",
    "# example run without using weights\n",
    "print k_nearest_neighbors(X_train[0], y_train, X_test[0], 5, weight=False)\n",
    "# example run using weights\n",
    "print k_nearest_neighbors(X_train, y_train, X_test[0], 5, weight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "INPUT:\n",
    "- training set features\n",
    "- training set classes\n",
    "- to be classified test set\n",
    "- value for k\n",
    "- weights (true or false)\n",
    "\n",
    "OUTPUT:\n",
    "- number of correctly classified instances\n",
    "- number of incorrectly classified instances\n",
    "\n",
    "In addition to returning values, the function also prettyprints the results\n",
    "'''\n",
    "def k_nn_evaluation(training_set_x, training_set_y, test_set_x, test_set_y, k, use_weight=False):    \n",
    "    correct = 0\n",
    "    false = 0\n",
    "\n",
    "    for i in range(len(test_set_x)):       \n",
    "        predicted = k_nearest_neighbors(training_set_x, training_set_y, test_set_x[i], k, use_weight)\n",
    "        actual = test_set_y[i][0]      \n",
    "        if predicted == actual:\n",
    "            correct += 1\n",
    "        if predicted != actual:\n",
    "            false +=1\n",
    " \n",
    "                \n",
    "                \n",
    "    accurracy = round((float(correct)/float(false + correct)), 5)\n",
    "    \n",
    "    print 'k =        ', k\n",
    "    print 'Correct:   ', correct\n",
    "    print 'False:     ', false\n",
    "    print 'Accurracy: ', accurracy\n",
    "    print ''\n",
    "    \n",
    "    return correct, false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results for unweighted k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =         1\n",
      "Correct:    237\n",
      "False:      3\n",
      "Accurracy:  0.9875\n",
      "\n",
      "k =         2\n",
      "Correct:    238\n",
      "False:      2\n",
      "Accurracy:  0.99167\n",
      "\n",
      "k =         3\n",
      "Correct:    236\n",
      "False:      4\n",
      "Accurracy:  0.98333\n",
      "\n",
      "k =         4\n",
      "Correct:    238\n",
      "False:      2\n",
      "Accurracy:  0.99167\n",
      "\n",
      "k =         5\n",
      "Correct:    236\n",
      "False:      4\n",
      "Accurracy:  0.98333\n",
      "\n",
      "k =         6\n",
      "Correct:    235\n",
      "False:      5\n",
      "Accurracy:  0.97917\n",
      "\n",
      "k =         7\n",
      "Correct:    235\n",
      "False:      5\n",
      "Accurracy:  0.97917\n",
      "\n",
      "k =         8\n",
      "Correct:    235\n",
      "False:      5\n",
      "Accurracy:  0.97917\n",
      "\n",
      "k =         9\n",
      "Correct:    234\n",
      "False:      6\n",
      "Accurracy:  0.975\n",
      "\n",
      "k =         10\n",
      "Correct:    235\n",
      "False:      5\n",
      "Accurracy:  0.97917\n",
      "\n",
      "Total correct:    2359\n",
      "Total false:      41\n",
      "Total accurracy:  0.98292\n"
     ]
    }
   ],
   "source": [
    "# find accurracy for k = 1, 2, ..., 10   \n",
    "total_correct = 0\n",
    "total_false = 0\n",
    "for i in range(10):        \n",
    "    correct, false = k_nn_evaluation(X_train, y_train, X_test, y_test, i+1)\n",
    "    total_correct += correct\n",
    "    total_false += false\n",
    "print 'Total correct:   ', total_correct\n",
    "print 'Total false:     ', total_false\n",
    "print 'Total accurracy: ', round((float(total_correct)/float(total_false+total_correct)), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results for weighted k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =         1\n",
      "Correct:    237\n",
      "False:      3\n",
      "Accurracy:  0.9875\n",
      "\n",
      "k =         2\n",
      "Correct:    237\n",
      "False:      3\n",
      "Accurracy:  0.9875\n",
      "\n",
      "k =         3\n",
      "Correct:    236\n",
      "False:      4\n",
      "Accurracy:  0.98333\n",
      "\n",
      "k =         4\n",
      "Correct:    236\n",
      "False:      4\n",
      "Accurracy:  0.98333\n",
      "\n",
      "k =         5\n",
      "Correct:    236\n",
      "False:      4\n",
      "Accurracy:  0.98333\n",
      "\n",
      "k =         6\n",
      "Correct:    235\n",
      "False:      5\n",
      "Accurracy:  0.97917\n",
      "\n",
      "k =         7\n",
      "Correct:    235\n",
      "False:      5\n",
      "Accurracy:  0.97917\n",
      "\n",
      "k =         8\n",
      "Correct:    235\n",
      "False:      5\n",
      "Accurracy:  0.97917\n",
      "\n",
      "k =         9\n",
      "Correct:    234\n",
      "False:      6\n",
      "Accurracy:  0.975\n",
      "\n",
      "k =         10\n",
      "Correct:    235\n",
      "False:      5\n",
      "Accurracy:  0.97917\n",
      "\n",
      "Total correct:    2356\n",
      "Total false:      44\n",
      "Total accurracy:  0.98167\n"
     ]
    }
   ],
   "source": [
    "# find accurracy for k = 1, 2, ..., 10   \n",
    "total_correct = 0\n",
    "total_false = 0\n",
    "for i in range(10):        \n",
    "    correct, false = k_nn_evaluation(X_train, y_train, X_test, y_test, i+1, use_weight=True)\n",
    "    total_correct += correct\n",
    "    total_false += false\n",
    "print 'Total correct:   ', total_correct\n",
    "print 'Total false:     ', total_false\n",
    "print 'Total accurracy: ', round((float(total_correct)/float(total_false+total_correct)), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion results weighted vs unweighted\n",
    "The differences between weighted k-NN and unweighted k-NN are minimal. Accurracy for weighted equals 0.98167, while accurracy for unweighted equals 0.98292. We expected the weighted implementation of k-NN to perform better, because the calculation is more sophisticated. \n",
    "\n",
    "\n",
    "\n",
    "After inspecting the euclidean distances, we found that (almost) all distances are in the same range. Therefore, the assigned weights will be roughly the same. As a result we think adding the weights did not improve the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1) C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for every feature, make list. combine everything in one list.\n",
    "single_features = []\n",
    "for i in range(len(X_train[0])):    \n",
    "    feature = []\n",
    "    for j in range(len(X_train)):\n",
    "        feature.append(X_train[j][i])\n",
    "    single_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- feature  1  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  2  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  3  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  4  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  5  ---\n",
      "k =         4\n",
      "Correct:    72\n",
      "False:      168\n",
      "Accurracy:  0.3\n",
      "\n",
      "--- feature  6  ---\n",
      "k =         4\n",
      "Correct:    72\n",
      "False:      168\n",
      "Accurracy:  0.3\n",
      "\n",
      "--- feature  7  ---\n",
      "k =         4\n",
      "Correct:    88\n",
      "False:      152\n",
      "Accurracy:  0.36667\n",
      "\n",
      "--- feature  8  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  9  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  10  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  11  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  12  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  13  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  14  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  15  ---\n",
      "k =         4\n",
      "Correct:    67\n",
      "False:      173\n",
      "Accurracy:  0.27917\n",
      "\n",
      "--- feature  16  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  17  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  18  ---\n",
      "k =         4\n",
      "Correct:    74\n",
      "False:      166\n",
      "Accurracy:  0.30833\n",
      "\n",
      "--- feature  19  ---\n",
      "k =         4\n",
      "Correct:    86\n",
      "False:      154\n",
      "Accurracy:  0.35833\n",
      "\n",
      "--- feature  20  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  21  ---\n",
      "k =         4\n",
      "Correct:    86\n",
      "False:      154\n",
      "Accurracy:  0.35833\n",
      "\n",
      "--- feature  22  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  23  ---\n",
      "k =         4\n",
      "Correct:    68\n",
      "False:      172\n",
      "Accurracy:  0.28333\n",
      "\n",
      "--- feature  24  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  25  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  26  ---\n",
      "k =         4\n",
      "Correct:    73\n",
      "False:      167\n",
      "Accurracy:  0.30417\n",
      "\n",
      "--- feature  27  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  28  ---\n",
      "k =         4\n",
      "Correct:    78\n",
      "False:      162\n",
      "Accurracy:  0.325\n",
      "\n",
      "--- feature  29  ---\n",
      "k =         4\n",
      "Correct:    72\n",
      "False:      168\n",
      "Accurracy:  0.3\n",
      "\n",
      "--- feature  30  ---\n",
      "k =         4\n",
      "Correct:    86\n",
      "False:      154\n",
      "Accurracy:  0.35833\n",
      "\n",
      "--- feature  31  ---\n",
      "k =         4\n",
      "Correct:    74\n",
      "False:      166\n",
      "Accurracy:  0.30833\n",
      "\n",
      "--- feature  32  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  33  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  34  ---\n",
      "k =         4\n",
      "Correct:    74\n",
      "False:      166\n",
      "Accurracy:  0.30833\n",
      "\n",
      "--- feature  35  ---\n",
      "k =         4\n",
      "Correct:    82\n",
      "False:      158\n",
      "Accurracy:  0.34167\n",
      "\n",
      "--- feature  36  ---\n",
      "k =         4\n",
      "Correct:    88\n",
      "False:      152\n",
      "Accurracy:  0.36667\n",
      "\n",
      "--- feature  37  ---\n",
      "k =         4\n",
      "Correct:    74\n",
      "False:      166\n",
      "Accurracy:  0.30833\n",
      "\n",
      "--- feature  38  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  39  ---\n",
      "k =         4\n",
      "Correct:    76\n",
      "False:      164\n",
      "Accurracy:  0.31667\n",
      "\n",
      "--- feature  40  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  41  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  42  ---\n",
      "k =         4\n",
      "Correct:    67\n",
      "False:      173\n",
      "Accurracy:  0.27917\n",
      "\n",
      "--- feature  43  ---\n",
      "k =         4\n",
      "Correct:    72\n",
      "False:      168\n",
      "Accurracy:  0.3\n",
      "\n",
      "--- feature  44  ---\n",
      "k =         4\n",
      "Correct:    86\n",
      "False:      154\n",
      "Accurracy:  0.35833\n",
      "\n",
      "--- feature  45  ---\n",
      "k =         4\n",
      "Correct:    74\n",
      "False:      166\n",
      "Accurracy:  0.30833\n",
      "\n",
      "--- feature  46  ---\n",
      "k =         4\n",
      "Correct:    72\n",
      "False:      168\n",
      "Accurracy:  0.3\n",
      "\n",
      "--- feature  47  ---\n",
      "k =         4\n",
      "Correct:    70\n",
      "False:      170\n",
      "Accurracy:  0.29167\n",
      "\n",
      "--- feature  48  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  49  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  50  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  51  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  52  ---\n",
      "k =         4\n",
      "Correct:    79\n",
      "False:      161\n",
      "Accurracy:  0.32917\n",
      "\n",
      "--- feature  53  ---\n",
      "k =         4\n",
      "Correct:    84\n",
      "False:      156\n",
      "Accurracy:  0.35\n",
      "\n",
      "--- feature  54  ---\n",
      "k =         4\n",
      "Correct:    88\n",
      "False:      152\n",
      "Accurracy:  0.36667\n",
      "\n",
      "--- feature  55  ---\n",
      "k =         4\n",
      "Correct:    82\n",
      "False:      158\n",
      "Accurracy:  0.34167\n",
      "\n",
      "--- feature  56  ---\n",
      "k =         4\n",
      "Correct:    74\n",
      "False:      166\n",
      "Accurracy:  0.30833\n",
      "\n",
      "--- feature  57  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  58  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  59  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  60  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  61  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  62  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  63  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n",
      "--- feature  64  ---\n",
      "k =         4\n",
      "Correct:    80\n",
      "False:      160\n",
      "Accurracy:  0.33333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store accurracy of features in array, later use this as weight\n",
    "feature_weights = np.array([])\n",
    "for i in range(len(single_features)):\n",
    "    print '--- feature ', i+1, ' ---'    \n",
    "    correct, false = k_nn_evaluation(single_features[i], y_train, X_test, y_test, 4)    \n",
    "    feature_weights = np.hstack([feature_weights, round((float(correct)/float(false+correct)), 5)])\n",
    "    \n",
    "# normalize weights\n",
    "normalizer = 1/sum(feature_weights)\n",
    "feature_weights *= normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Very similar to 'k_nearest_neighbors', but with an additional parameter for weights.\n",
    "INPUT:\n",
    "- training set X (training examples with features)\n",
    "- training set y (classes corresponding to features)\n",
    "- test_instance (to be classified instance)\n",
    "- k (determines how many neighbors are taken into account)\n",
    "- weights (vector with predictive values of features)\n",
    "\n",
    "OUTPUT:\n",
    "- class assignment for test instance\n",
    "'''\n",
    "def k_nearest_neighbors_featureweight(training_set_x, training_set_y, test_instance, k, weights):    \n",
    "    distances = []  \n",
    "    result = 0\n",
    "\n",
    "    for i in range(len(training_set_x)):\n",
    "        w_feature_distance = 0\n",
    "        for j in range(len(weights)):\n",
    "            w_feature_distance += distance(training_set_x[i] * weights, test_instance)            \n",
    "        distances.append((i, w_feature_distance))\n",
    "\n",
    "    # sort list: ascending distances\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    \n",
    "    prediction = 0              \n",
    "    for i in range(k):\n",
    "        prediction += training_set_y[distances[:k][i][0]][0]\n",
    "                               \n",
    "    return round(prediction/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "INPUT:\n",
    "- training set features\n",
    "- training set classes\n",
    "- to be classified test set\n",
    "- value for k\n",
    "- weights (vector with predictive values of features)\n",
    "\n",
    "OUTPUT:\n",
    "- number of correctly classified instances\n",
    "- number of incorrectly classified instances\n",
    "\n",
    "In addition to returning values, the function also prettyprints the results\n",
    "'''\n",
    "def k_nn_evaluation_featureweight(training_set_x, training_set_y, test_set_x, test_set_y, k, weights):    \n",
    "    correct = 0\n",
    "    false = 0\n",
    "    \n",
    "    for i in range(len(test_set_x)):       \n",
    "        predicted = k_nearest_neighbors_featureweight(training_set_x, training_set_y, test_set_x[i], k, weights)\n",
    "        actual = test_set_y[i][0]      \n",
    "        if predicted == actual:\n",
    "            correct += 1\n",
    "        if predicted != actual:\n",
    "            false +=1\n",
    "                 \n",
    "    accurracy = round((float(correct)/float(false + correct)), 5)\n",
    "    \n",
    "    print 'k =        ', k\n",
    "    print 'Correct:   ', correct\n",
    "    print 'False:     ', false\n",
    "    print 'Accurracy: ', accurracy\n",
    "    print ''\n",
    "    \n",
    "    return correct, false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-93a04143f5b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_false\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfalse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_nn_evaluation_featureweight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_correct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_false\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2c1e92e9cafe>\u001b[0m in \u001b[0;36mk_nn_evaluation_featureweight\u001b[0;34m(training_set_x, training_set_y, test_set_x, test_set_y, k, weights)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_nearest_neighbors_featureweight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_set_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mactual\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7485d06b4f44>\u001b[0m in \u001b[0;36mk_nearest_neighbors_featureweight\u001b[0;34m(training_set_x, training_set_y, test_instance, k, weights)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mw_feature_distance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mw_feature_distance\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_instance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_feature_distance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-05ed0926317e>\u001b[0m in \u001b[0;36mdistance\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      8\u001b[0m '''\n\u001b[1;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m '''\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# find accurracy for k = 1, 2, ..., 10   \n",
    "total_correct = 0\n",
    "total_false = 0\n",
    "for i in range(10):        \n",
    "    correct, false = k_nn_evaluation_featureweight(X_train, y_train, X_test, y_test, i+1, feature_weights)\n",
    "    total_correct += correct\n",
    "    total_false += false\n",
    "print 'Total correct:   ', total_correct\n",
    "print 'Total false:     ', total_false\n",
    "print 'Total accurracy: ', round((float(total_correct)/float(total_false+total_correct)), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion results weighted features\n",
    "\n",
    "Above results show that the weighing of features performs worse than using distance weight or no weight. Accurracy using weighted features is 79.7%, which is significantly worse than the other two implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forward(X, W, W_output):    \n",
    "    steps = list()\n",
    "    steps.append(X)    \n",
    "    # loop over weights for hidden layers\n",
    "    for i in range(len(W)):\n",
    "        steps.append(sigmoid(np.dot(steps[i], W[i])))    \n",
    "    # for final value, use W_output\n",
    "    steps.append(sigmoid(np.dot(steps[-1], W_output)))\n",
    "    \n",
    "    return steps, steps[-1]\n",
    "\n",
    "def weights(inputLayerSize, outputLayerSize, hiddenLayerSize, hiddenLayerDepth, value):    \n",
    "    W = list()\n",
    "    W_output = 0\n",
    "    \n",
    "    if value == -1:\n",
    "        # create random weight values for all hidden layers\n",
    "        W = list()\n",
    "        W.append(np.random.randn(inputLayerSize, hiddenLayerSize))\n",
    "        if hiddenLayerDepth > 1:\n",
    "            for i in range(hiddenLayerDepth):\n",
    "                W.append(np.random.randn(hiddenLayerSize, hiddenLayerSize))\n",
    "\n",
    "        # W_output might have different dimensions \n",
    "        # (if outputLayerSize != hiddenLayerSize)\n",
    "        W_output = np.random.randn(hiddenLayerSize, outputLayerSize)\n",
    "    \n",
    "    if value != -1:\n",
    "        W = list()\n",
    "        W.append(np.full((inputLayerSize, hiddenLayerSize), value))\n",
    "        if hiddenLayerDepth > 1:\n",
    "            for i in range(hiddenLayerDepth):\n",
    "                W.append(np.full((hiddenLayerSize, hiddenLayerSize), value))\n",
    "            \n",
    "        # W_output might have different dimensions \n",
    "        # (if outputLayerSize != hiddenLayerSize)\n",
    "        W_output = np.full((hiddenLayerSize, outputLayerSize), value)\n",
    "        \n",
    "    return W, W_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_logistic_data(filename):\n",
    "    my_data = genfromtxt(filename, delimiter=';')\n",
    "    X = []\n",
    "    y= []\n",
    "    for i in range(len(my_data)):\n",
    "        new_data = my_data[i]\n",
    "        y.append([new_data[-1]])\n",
    "        \n",
    "        \n",
    "        X.append(np.delete(new_data, -1))\n",
    "        \n",
    "        \n",
    "        #X.append(np.delete(new_data, -1))\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "X, temp_y = read_logistic_data('digits123-1.csv')\n",
    "\n",
    "'''\n",
    "for this specific handwritten digit classification problem, we need to have \n",
    "three classes: 1, 2, 3. Representation of the classes will be as follows:\n",
    "\n",
    "class '1' =   class '2' =   class '3' =\n",
    "[1,           [0,           [0,\n",
    " 0,            1,            0,\n",
    " 0]            0]            1]\n",
    "'''\n",
    "y = []\n",
    "\n",
    "for i in range(len(temp_y)):\n",
    "    if temp_y[i][0] == 1.0:\n",
    "        y.append(np.array([[1, 0, 0]]))        \n",
    "    if temp_y[i][0] == 2.0:\n",
    "        y.append(np.array([[0, 1, 0]]))      \n",
    "    if temp_y[i][0] == 3.0:\n",
    "        y.append(np.array([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chosen parameters for neural network:\n",
    "inputLayerSize = 64   # 64 features\n",
    "outputLayerSize = 3   # three classes: 1, 2, 3\n",
    "hiddenLayerSize = 35  # simple rule of thumb: n of neurons in hidden layer\n",
    "                      # is mean of inputLayerSize and outPutlayerSize.\n",
    "hiddenLayerDepth = 1\n",
    "\n",
    "# create weights for NN using above parameters\n",
    "W, W_output = weights(inputLayerSize, outputLayerSize, hiddenLayerSize, hiddenLayerDepth, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagation(X, y, W, W_output, alpha, iterations):\n",
    "    count = 0    \n",
    "    dy_dtheta1 = 0\n",
    "    dy_dtheta2 = 0   \n",
    "    while count < iterations:\n",
    "        # loop over every training example. After loop, update weights.\n",
    "        for i in range(len(X)):\n",
    "            steps, resultY = forward(X[i], W, W_output)    \n",
    "\n",
    "            # find delta_3: hypothesis - actual answer\n",
    "            d_3 = resultY-y[i]       \n",
    "            # find delta_2: ((output weights) . d_3 ) * g'(z(2))\n",
    "            d_2 = np.dot(W_output, d_3[0].T) * sigmoid_derivative(steps[-2])\n",
    "            \n",
    "            # 3 outgoing arrows from hidden layer to final layer\n",
    "            # [hidden] --> [hidden, hidden, hidden]\n",
    "            # used for vectorization of code\n",
    "            l2 = steps[-2].reshape(35,1)\n",
    "            l2 = np.concatenate((l2, l2, l2), axis=1)        \n",
    "                       \n",
    "            # every input data point needs to connect to 35 hidden layers\n",
    "            # [input] --> [input, input, ..., input] (width = 35 elements)\n",
    "            l1 = steps[-3].reshape(64,1)\n",
    "            for i in range(5):\n",
    "                l1 = np.concatenate((l1, l1), axis = 1)    \n",
    "            l1 = np.concatenate((l1, steps[-3].reshape(64,1), steps[-3].reshape(64,1), steps[-3].reshape(64,1)), axis=1)\n",
    "            \n",
    "            # add derivative of training example i\n",
    "            dy_dtheta2 += l2*d_3\n",
    "            dy_dtheta1 += l1*d_2\n",
    "\n",
    "        # update weights\n",
    "        W_updated = list()\n",
    "        W_updated.append((1.0/len(X))*(W[0] - alpha * dy_dtheta1))\n",
    "        W_output_updated = (1.0/len(X)) * (W_output - alpha * dy_dtheta2)\n",
    "        \n",
    "        W = W_updated\n",
    "        W_output = W_output_updated\n",
    "        \n",
    "        count = count+1\n",
    "  \n",
    "    return W_updated, W_output_updated\n",
    "        \n",
    "    \n",
    "W_resulting, W_output_resulting = backpropagation(X, y, W, W_output, 0.04, 500)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_class(results):   \n",
    "    \n",
    "    if ((results[0] > results[1]) & (results[0] > results[2])):\n",
    "        number = 1.0\n",
    "    if ((results[1] > results[0]) & (results[1] > results[2])):\n",
    "        number = 2.0\n",
    "    if ((results[2] > results[0]) & (results[2] > results[1])):\n",
    "        number = 3.0\n",
    "\n",
    "    return number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy logistic regression scikit:  0.780371206145\n",
      "Accuracy k-nearest scikit:  0.979166666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yvonn\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Dit moet je ff van te voren in je terminal runnen.\n",
    "#pip install scikit-neuralnetwork\n",
    "#pip install scikit-learn==0.18rc2\n",
    "\n",
    "#Logistic regression\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "#Normal accuracy logistic regression\n",
    "reg.fit(X_train, y_train)\n",
    "print 'Accuracy logistic regression scikit: ', reg.score(X_test, y_test)\n",
    "\n",
    "#Backpropagation\n",
    "clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=1000)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#K-nearest neighbour classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#Normal accuracy neighbour\n",
    "neigh.fit(X_train, y_train)\n",
    "print 'Accuracy k-nearest scikit: ', neigh.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full test set digits123-2 used:\n",
      "n correct:    228\n",
      "n incorrect:  12\n",
      "Accurracy our neural network:    0.95\n",
      "Accuracy neural network scikit:  0.904166666667\n",
      "240\n",
      "Test set digits123-2 split in 2 for cross validation:\n",
      "Accurracy our neural network cross set:    0.933333333333\n",
      "Accurracy our neural network test set:    0.966666666667\n",
      "Accuracy neural network scikit cross set:  0.916666666667\n",
      "Accuracy neural network scikit test set:  0.891666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_test, y_test = read_logistic_data('digits123-2.csv')\n",
    "steps, resultY = forward(X_test, W_resulting, W_output_resulting)    \n",
    "\n",
    "\n",
    "def correctness(resultY, y_test):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(resultY)):\n",
    "        if find_class(resultY[i]) == y_test[i][0]:\n",
    "            correct += 1\n",
    "        if find_class(resultY[i]) != y_test[i][0]:\n",
    "            incorrect +=1\n",
    "    return correct, incorrect\n",
    "\n",
    "print 'Full test set digits123-2 used:'\n",
    "correct, incorrect = correctness(resultY, y_test)\n",
    "print 'n correct:   ', correct\n",
    "print 'n incorrect: ', incorrect\n",
    "print 'Accurracy our neural network:   ', float(correct)/(float(correct)+float(incorrect))\n",
    "print 'Accuracy neural network scikit: ', clf.score(X_test, y_test)\n",
    "print(len(X_test))\n",
    "\n",
    "print 'Test set digits123-2 split in 2 for cross validation:'\n",
    "def cross_validation(X_test, y_test):\n",
    "    X_cross = X_test[:120]\n",
    "    new_X_test = X_test[120:]\n",
    "    y_cross = y_test[:120]\n",
    "    new_y_test = y_test[120:]\n",
    "    steps_cross, resultY_cross = forward(X_cross, W_resulting, W_output_resulting)\n",
    "    steps_test, resultY_test = forward(new_X_test, W_resulting, W_output_resulting)\n",
    "    correct_cross, incorrect_cross = correctness(resultY_cross, y_cross)\n",
    "    correct_test, incorrect_test = correctness(resultY_test, new_y_test)\n",
    "    print 'Accurracy our neural network cross set:   ', float(correct_cross)/(float(correct_cross)+float(incorrect_cross))\n",
    "    print 'Accurracy our neural network test set:   ', float(correct_test)/(float(correct_test)+float(incorrect_test))\n",
    "    print 'Accuracy neural network scikit cross set: ', clf.score(X_cross, y_cross)\n",
    "    print 'Accuracy neural network scikit test set: ', clf.score(new_X_test, new_y_test)\n",
    "cross_validation(X_test, y_test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Discussion question 2\n",
    "\n",
    "Above results show the accuracy of our neural network and the neural network that is used by the scikit-learn package. It is shown that our accuracy is better. It is noticable that with cross validation that our neural network has a better accuracy in both sets. So we chose to try and optimize our own neural network even further for a better accuracy value.\n",
    "\n",
    "First we tried to normalize our training and test set to try and retrieve better results. Sadly this was not the case, because the accuracy of our model decreased. After that we tried to use different weights for our neural network. Like 0.001, 0.01 and 0.1. This also resulted in a decrease in accuracy. After that we looked at the wrong predictions that were made by our neural network. It was noticable that the most errors that were made by our neural network were a wrong prediction of class 2. The neural network predicted that the digits belonged to class 1, when they actually belonged to class 2. We tried to introduce a bias for class 2, unfortunately this was unsuccesfull.\n",
    "\n",
    "With this being said. We did not know how we could improve our neural network any further. So our final accuracy is 0.95. We are very pleased by this result."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
